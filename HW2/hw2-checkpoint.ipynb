{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYlaRwNu7ojq"
   },
   "source": [
    "# **Homework 2-1 Phoneme Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emUd7uS7crTz"
   },
   "source": [
    "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
    "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
    "\n",
    "This homework is a multiclass classification task, \n",
    "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
    "\n",
    "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVUGfWTo7_Oj"
   },
   "source": [
    "## Download Data\n",
    "Download data from google drive, then unzip it.\n",
    "\n",
    "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
    "`timit_11/`\n",
    "- `train_11.npy`: training data<br>\n",
    "- `train_label_11.npy`: training label<br>\n",
    "- `test_11.npy`:  testing data<br><br>\n",
    "\n",
    "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y114Vmm3Ja6o"
   },
   "outputs": [],
   "source": [
    "#check device\n",
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us5XW_x6udZQ"
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Fjf5EcmJtf4e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TIMITDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(np.int)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L_4anls8Drv"
   },
   "source": [
    "## Preparing Data\n",
    "Load the training and testing data from the `.npy` file (NumPy array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otIC6WhGeh9v"
   },
   "source": [
    "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJjLT8em-y9G",
    "outputId": "8edc6bfe-7511-447f-f239-00b96dba6dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (1229932, 546)\n",
      "Size of training set: (983945, 546)\n",
      "Size of validation set: (245987, 546)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "data_root='./timit_11/'\n",
    "\n",
    "SKIP_FRAME = 4\n",
    "SHUFFLE = True\n",
    "\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_first = train[:, :-39*SKIP_FRAME]\n",
    "# train_mid = train[:, 39*4:-39*4]\n",
    "train_last = train[:, 39*SKIP_FRAME:]\n",
    "train = np.c_[train_first, train_last]\n",
    "del train_first, train_last\n",
    "\n",
    "print('Size of training data: {}'.format(train.shape))\n",
    "\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "\n",
    "VAL_RATIO = 0.1\n",
    "# SELF_TEST_SET_RATIO = 0.1\n",
    "'''\n",
    "# TODO: \n",
    "    (1) self test set\n",
    "    (2) corss-fold valid\n",
    "'''\n",
    "\n",
    "if SHUFFLE:\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train, train_label, test_size=VAL_RATIO, random_state=1126)\n",
    "else:\n",
    "    percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "    train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "\n",
    "print('Size of training set: {}'.format(train_x.shape))\n",
    "print('Size of validation set: {}'.format(val_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_counter_hist(y):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    val_cnt = Counter(y).items()\n",
    "    print(len(val_cnt))\n",
    "    val_cnt = [(int(e[0]), e[1]) for e in val_cnt]\n",
    "    val_cnt = sorted(val_cnt, key=itemgetter(0))\n",
    "    val_cnt = Counter(dict(val_cnt))\n",
    "    plt.bar(val_cnt.keys(), val_cnt.values())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_counter_hist(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFlCAYAAABFpfSEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAaAklEQVR4nO3df6yeZZ3n8fdnCigZx22Rs4Rt65bVZk0la9UOdKLZMBihoNli4hrYXeka1joREk3cWYvZBH+R4B/KjImSZZYOZaMi8cfSaF2mQTauf/CjaAUKGs5gDW0qrRZEYxYD890/nqvrM/Wcnqc/zrnOOc/7lTw59/29r/t+rufK3Z7PuX89qSokSZLUzx/17oAkSdK4M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZ6f17sCJOvvss2vVqlW9uyFJkjSjhx9++BdVNTHd8gUbyFatWsWuXbt6d0OSJGlGSX52rOWespQkSerMQCZJktSZgUySJKkzA5kkSVJnBjJJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzk7r3QFJkqRTbdWWb4/Ubu9N75jlnozGI2SSJEmdGcgkSZI6mzGQJXl5kgeT/CjJniSfaPXbk/w0ye72WtvqSfL5JJNJHknypqFtbUryZHttGqq/OcmjbZ3PJ8ksfFZJkqR5aZRryF4ALq6q3yQ5Hfh+ku+0ZX9ZVV87qv1lwOr2uhC4BbgwyVnADcA6oICHk2yvqmdbm/cDDwA7gA3Ad5AkSRoDMx4hq4HftNnT26uOscpG4I623v3A0iTnApcCO6vqcAthO4ENbdkrq+r+qirgDuCKE/9IkiRJC8tI15AlWZJkN3CQQah6oC26sZ2WvDnJy1ptOfD00Or7Wu1Y9X1T1Kfqx+Yku5LsOnTo0ChdlyRJmvdGCmRV9VJVrQVWABckOR+4Hngd8KfAWcBHZ6uTQ/24tarWVdW6iYmJ2X47SZKkOXFcd1lW1XPAfcCGqjrQTku+APwtcEFrth9YObTailY7Vn3FFHVJkqSxMMpdlhNJlrbpM4G3Az9u137R7oi8AnisrbIduLrdbbke+FVVHQDuAS5JsizJMuAS4J627Pkk69u2rgbuPpUfUpIkaT4b5S7Lc4FtSZYwCHB3VdW3knw3yQQQYDfwF639DuByYBL4LfA+gKo6nORTwEOt3Ser6nCb/iBwO3Amg7srvcNSkiSNjRkDWVU9ArxxivrF07Qv4Npplm0Ftk5R3wWcP1NfJEmSFiOf1C9JktSZgUySJKkzA5kkSVJnBjJJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sxAJkmS1JmBTJIkqTMDmSRJUmcGMkmSpM4MZJIkSZ0ZyCRJkjozkEmSJHVmIJMkSerMQCZJktSZgUySJKkzA5kkSVJnBjJJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpsxkDWZKXJ3kwyY+S7EnyiVY/L8kDSSaTfDXJGa3+sjY/2ZavGtrW9a3+kySXDtU3tNpkki2z8DklSZLmrVGOkL0AXFxVbwDWAhuSrAc+A9xcVa8FngWuae2vAZ5t9ZtbO5KsAa4EXg9sAL6YZEmSJcAXgMuANcBVra0kSdJYmDGQ1cBv2uzp7VXAxcDXWn0bcEWb3tjmacvfliStfmdVvVBVPwUmgQvaa7Kqnqqq3wF3traSJEljYaRryNqRrN3AQWAn8PfAc1X1YmuyD1jeppcDTwO05b8CXjVcP2qd6eqSJEljYaRAVlUvVdVaYAWDI1qvm81OTSfJ5iS7kuw6dOhQjy5IkiSdcsd1l2VVPQfcB/wZsDTJaW3RCmB/m94PrARoy/8J8Mvh+lHrTFef6v1vrap1VbVuYmLieLouSZI0b41yl+VEkqVt+kzg7cATDILZu1uzTcDdbXp7m6ct/25VVatf2e7CPA9YDTwIPASsbndtnsHgwv/tp+CzSZIkLQinzdyEc4Ft7W7IPwLuqqpvJXkcuDPJp4EfAre19rcB/yPJJHCYQcCiqvYkuQt4HHgRuLaqXgJIch1wD7AE2FpVe07ZJ5QkSZrnZgxkVfUI8MYp6k8xuJ7s6Pr/Bf7tNNu6EbhxivoOYMcI/ZUkSVp0fFK/JElSZwYySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sxAJkmS1JmBTJIkqTMDmSRJUmcGMkmSpM4MZJIkSZ0ZyCRJkjozkEmSJHVmIJMkSerMQCZJktSZgUySJKkzA5kkSVJnBjJJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sxAJkmS1JmBTJIkqTMDmSRJUmcGMkmSpM4MZJIkSZ3NGMiSrExyX5LHk+xJ8qFW/3iS/Ul2t9flQ+tcn2QyyU+SXDpU39Bqk0m2DNXPS/JAq381yRmn+oNKkiTNV6McIXsR+EhVrQHWA9cmWdOW3VxVa9trB0BbdiXwemAD8MUkS5IsAb4AXAasAa4a2s5n2rZeCzwLXHOKPp8kSdK8N2Mgq6oDVfWDNv1r4Alg+TFW2QjcWVUvVNVPgUnggvaarKqnqup3wJ3AxiQBLga+1tbfBlxxgp9HkiRpwTmua8iSrALeCDzQStcleSTJ1iTLWm058PTQavtabbr6q4DnqurFo+pTvf/mJLuS7Dp06NDxdF2SJGneGjmQJXkF8HXgw1X1PHAL8BpgLXAA+OxsdHBYVd1aVeuqat3ExMRsv50kSdKcOG2URklOZxDGvlRV3wCoqmeGlv8N8K02ux9YObT6ilZjmvovgaVJTmtHyYbbS5IkLXqj3GUZ4Dbgiar63FD93KFm7wIea9PbgSuTvCzJecBq4EHgIWB1u6PyDAYX/m+vqgLuA97d1t8E3H1yH0uSJGnhGOUI2VuA9wKPJtndah9jcJfkWqCAvcAHAKpqT5K7gMcZ3KF5bVW9BJDkOuAeYAmwtar2tO19FLgzyaeBHzIIgJIkSWNhxkBWVd8HMsWiHcdY50bgxinqO6Zar6qeYnAXpiRJ0tjxSf2SJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sxAJkmS1JmBTJIkqTMDmSRJUmcGMkmSpM4MZJIkSZ0ZyCRJkjozkEmSJHVmIJMkSerMQCZJktSZgUySJKkzA5kkSVJnBjJJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnRnIJEmSOpsxkCVZmeS+JI8n2ZPkQ61+VpKdSZ5sP5e1epJ8PslkkkeSvGloW5ta+yeTbBqqvznJo22dzyfJbHxYSZKk+WiUI2QvAh+pqjXAeuDaJGuALcC9VbUauLfNA1wGrG6vzcAtMAhwwA3AhcAFwA1HQlxr8/6h9Tac/EeTJElaGGYMZFV1oKp+0KZ/DTwBLAc2Attas23AFW16I3BHDdwPLE1yLnApsLOqDlfVs8BOYENb9sqqur+qCrhjaFuSJEmL3nFdQ5ZkFfBG4AHgnKo60Bb9HDinTS8Hnh5abV+rHau+b4q6JEnSWBg5kCV5BfB14MNV9fzwsnZkq05x36bqw+Yku5LsOnTo0Gy/nSRJ0pwYKZAlOZ1BGPtSVX2jlZ9ppxtpPw+2+n5g5dDqK1rtWPUVU9T/QFXdWlXrqmrdxMTEKF2XJEma90a5yzLAbcATVfW5oUXbgSN3Sm4C7h6qX93utlwP/Kqd2rwHuCTJsnYx/yXAPW3Z80nWt/e6emhbkiRJi95pI7R5C/Be4NEku1vtY8BNwF1JrgF+BrynLdsBXA5MAr8F3gdQVYeTfAp4qLX7ZFUdbtMfBG4HzgS+016SJEljYcZAVlXfB6Z7LtjbpmhfwLXTbGsrsHWK+i7g/Jn6IkmStBj5pH5JkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sxAJkmS1JmBTJIkqTMDmSRJUmcGMkmSpM4MZJIkSZ0ZyCRJkjozkEmSJHVmIJMkSerMQCZJktTZab07IEmSFqZVW749Uru9N71jlnuy8BnITrFRd05wB5UkSQOespQkSerMQCZJktSZgUySJKkzA5kkSVJnBjJJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpsxkDWZKtSQ4meWyo9vEk+5Psbq/Lh5Zdn2QyyU+SXDpU39Bqk0m2DNXPS/JAq381yRmn8gNKkiTNd6McIbsd2DBF/eaqWtteOwCSrAGuBF7f1vlikiVJlgBfAC4D1gBXtbYAn2nbei3wLHDNyXwgSZKkhWbGQFZV3wMOj7i9jcCdVfVCVf0UmAQuaK/Jqnqqqn4H3AlsTBLgYuBrbf1twBXH9xEkSZIWtpO5huy6JI+0U5rLWm058PRQm32tNl39VcBzVfXiUfUpJdmcZFeSXYcOHTqJrkuSJM0fJxrIbgFeA6wFDgCfPVUdOpaqurWq1lXVuomJibl4S0mSpFl32omsVFXPHJlO8jfAt9rsfmDlUNMVrcY09V8CS5Oc1o6SDbeXJEkaCycUyJKcW1UH2uy7gCN3YG4Hvpzkc8A/A1YDDwIBVic5j0HguhL4d1VVSe4D3s3gurJNwN0n+mG0sK3a8u2R2u296R2z3BNJkubWjIEsyVeAi4Czk+wDbgAuSrIWKGAv8AGAqtqT5C7gceBF4Nqqeqlt5zrgHmAJsLWq9rS3+ChwZ5JPAz8EbjtVH06SJGkhmDGQVdVVU5SnDU1VdSNw4xT1HcCOKepPMbgLU5IkaSz5pH5JkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sxAJkmS1JmBTJIkqTMDmSRJUmen9e7AfLZqy7dHbrv3pnfMYk8kSdJi5hEySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sy7LBco7wCVJGnx8AiZJElSZwYySZKkzgxkkiRJnRnIJEmSOjOQSZIkdWYgkyRJ6sxAJkmS1JmBTJIkqTMDmSRJUmcGMkmSpM4MZJIkSZ3N+F2WSbYC7wQOVtX5rXYW8FVgFbAXeE9VPZskwF8DlwO/Bf5jVf2grbMJ+K9ts5+uqm2t/mbgduBMYAfwoaqqU/T5JC0wfk+rpHE0yhGy24ENR9W2APdW1Wrg3jYPcBmwur02A7fA/w9wNwAXAhcANyRZ1ta5BXj/0HpHv5ckSdKiNmMgq6rvAYePKm8EtrXpbcAVQ/U7auB+YGmSc4FLgZ1VdbiqngV2AhvasldW1f3tqNgdQ9uSJEkaCyd6Ddk5VXWgTf8cOKdNLweeHmq3r9WOVd83RV2SJGlsnPRF/e3I1pxc85Vkc5JdSXYdOnRoLt5SkiRp1p1oIHumnW6k/TzY6vuBlUPtVrTaseorpqhPqapurap1VbVuYmLiBLsuSZI0v5xoINsObGrTm4C7h+pXZ2A98Kt2avMe4JIky9rF/JcA97RlzydZ3+7QvHpoW5IkSWNhlMdefAW4CDg7yT4Gd0veBNyV5BrgZ8B7WvMdDB55McngsRfvA6iqw0k+BTzU2n2yqo7cKPBBfv/Yi++0lyRJ0tiYMZBV1VXTLHrbFG0LuHaa7WwFtk5R3wWcP1M/JOlU8nlnkuYTn9QvSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnRnIJEmSOpvxsReS5rdRH9/goxskaf7yCJkkSVJnBjJJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzMdeSGNo1EdlgI/LkKS5YCAbI/4SliRpfvKUpSRJUmcGMkmSpM4MZJIkSZ0ZyCRJkjozkEmSJHXmXZbzgHc/SpI03jxCJkmS1JmBTJIkqTNPWWrsjHqK2NPDkqS54hEySZKkzgxkkiRJnXnKUpKkOeAd9ToWj5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkznzshRY0n7ovSVoMDGSSJGnO+If01E7qlGWSvUkeTbI7ya5WOyvJziRPtp/LWj1JPp9kMskjSd40tJ1Nrf2TSTad3EeSJElaWE7FEbI/r6pfDM1vAe6tqpuSbGnzHwUuA1a314XALcCFSc4CbgDWAQU8nGR7VT17Cvqmk+STpeeWfzlK0niajVOWG4GL2vQ24H8zCGQbgTuqqoD7kyxNcm5ru7OqDgMk2QlsAL4yC32TJOmk+ceqTrWTDWQF/F2SAv5bVd0KnFNVB9rynwPntOnlwNND6+5rtenqfyDJZmAzwKtf/eqT7LqkxcJfjpIWupMNZG+tqv1J/imwM8mPhxdWVbWwdkq0wHcrwLp1607ZdiVJkno6qYv6q2p/+3kQ+CZwAfBMOxVJ+3mwNd8PrBxafUWrTVeXJEkaCyccyJL8cZI/OTINXAI8BmwHjtwpuQm4u01vB65ud1uuB37VTm3eA1ySZFm7I/OSVpMkSRoLJ3PK8hzgm0mObOfLVfW/kjwE3JXkGuBnwHta+x3A5cAk8FvgfQBVdTjJp4CHWrtPHrnAX5IkHT/v2F54TjiQVdVTwBumqP8SeNsU9QKunWZbW4GtJ9oXSZKkhczvspQkSerMr06SJI01H5ui+cBAJmnW+ItOkkbjKUtJkqTODGSSJEmdGcgkSZI6M5BJkiR1ZiCTJEnqzEAmSZLUmYFMkiSpMwOZJElSZwYySZKkzgxkkiRJnfnVSZIkaV4b9WvYFvJXsHmETJIkqTMDmSRJUmeespQkaZ4a9VQdLOzTdTKQSRqRvxgkafYYyKQRnMgFpeNwEao03/iHgxYqryGTJEnqzCNkkrRIeFRWWrg8QiZJktSZgUySJKkzT1lK0nHwtKCk2WAgkyQdF0OpdOoZyCRpHlpsoWexfR7pVDOQSdIsM4xoIXA/7ctAJkmal3zIq8aJd1lKkiR1ZiCTJEnqzEAmSZLUmdeQaVZ4cai0MPhvVZofPEImSZLUmYFMkiSps3kTyJJsSPKTJJNJtvTujyRJ0lyZF4EsyRLgC8BlwBrgqiRr+vZKkiRpbsyLQAZcAExW1VNV9TvgTmBj5z5JkiTNifkSyJYDTw/N72s1SZKkRS9V1bsPJHk3sKGq/lObfy9wYVVdd1S7zcDmNvsvgZ/MaUcHzgZ+0eF95xvHwTEAx+AIx8ExOMJxcAxg6jH451U1Md0K8+U5ZPuBlUPzK1rtH6mqW4Fb56pTU0myq6rW9ezDfOA4OAbgGBzhODgGRzgOjgGc2BjMl1OWDwGrk5yX5AzgSmB75z5JkiTNiXlxhKyqXkxyHXAPsATYWlV7OndLkiRpTsyLQAZQVTuAHb37MYKup0znEcfBMQDH4AjHwTE4wnFwDOAExmBeXNQvSZI0zubLNWSSJEljy0B2HPx6J0iyN8mjSXYn2dW7P3MlydYkB5M8NlQ7K8nOJE+2n8t69nG2TTMGH0+yv+0Pu5Nc3rOPsy3JyiT3JXk8yZ4kH2r1cdsXphuHsdkfkrw8yYNJftTG4BOtfl6SB9rvia+2G9UWpWOMwe1Jfjq0H6zt3NVZl2RJkh8m+VabP+79wEA2Ir/e6R/586paO2a3Nd8ObDiqtgW4t6pWA/e2+cXsdv5wDABubvvD2nYt6GL2IvCRqloDrAeubf8PjNu+MN04wPjsDy8AF1fVG4C1wIYk64HPMBiD1wLPAtf06+Ksm24MAP5yaD/Y3auDc+hDwBND88e9HxjIRufXO42xqvoecPio8kZgW5veBlwxl32aa9OMwVipqgNV9YM2/WsG/wEvZ/z2henGYWzUwG/a7OntVcDFwNdafVHvC8cYg7GSZAXwDuC/t/lwAvuBgWx0fr3TQAF/l+Th9s0J4+ycqjrQpn8OnNOzMx1dl+SRdkpzUZ+qG5ZkFfBG4AHGeF84ahxgjPaHdppqN3AQ2An8PfBcVb3Ymiz63xNHj0FVHdkPbmz7wc1JXtavh3Pir4D/AvxDm38VJ7AfGMh0vN5aVW9icOr22iT/uneH5oMa3K48dn8ZArcAr2FwuuIA8NmuvZkjSV4BfB34cFU9P7xsnPaFKcZhrPaHqnqpqtYy+HaZC4DX9e3R3Dt6DJKcD1zPYCz+FDgL+Gi/Hs6uJO8EDlbVwye7LQPZ6Eb6eqfFrqr2t58HgW8y+E9oXD2T5FyA9vNg5/7Muap6pv2H/A/A3zAG+0OS0xmEkC9V1Tdaeez2hanGYRz3B4Cqeg64D/gzYGmSI8/4HJvfE0NjsKGd0q6qegH4Wxb3fvAW4N8k2cvgUqaLgb/mBPYDA9noxv7rnZL8cZI/OTINXAI8duy1FrXtwKY2vQm4u2NfujgSQpp3scj3h3ZtyG3AE1X1uaFFY7UvTDcO47Q/JJlIsrRNnwm8ncG1dPcB727NFvW+MM0Y/Hjoj5MwuHZq0e4HVXV9Va2oqlUMcsF3q+rfcwL7gQ+GPQ7tFu6/4vdf73Rj3x7NrST/gsFRMRh8y8OXx2UMknwFuAg4G3gGuAH4n8BdwKuBnwHvqapFe9H7NGNwEYPTUwXsBT4wdC3VopPkrcD/AR7l99eLfIzB9VPjtC9MNw5XMSb7Q5J/xeBi7SUMDm7cVVWfbP9P3sngVN0Pgf/QjhQtOscYg+8CE0CA3cBfDF38v2gluQj4z1X1zhPZDwxkkiRJnXnKUpIkqTMDmSRJUmcGMkmSpM4MZJIkSZ0ZyCRJkjozkEmSJHVmIJMkSerMQCZJktTZ/wOxYaiE0TaDFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_counter_hist(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983945\n",
      "[50241, 67005, 27981, 47259, 31194, 21097, 3248, 59093, 23161, 27500, 8802, 9386, 21470, 34689, 31702, 8953, 16836, 41133, 19864, 37533, 6804, 5681, 5635, 8570, 3095, 6581, 6302, 4854, 9251, 16816, 19976, 25215, 9527, 18374, 5531, 67598, 21642, 11339, 143007]\n",
      "[0.9489392191636727, 0.9319016814964252, 0.9715624348921942, 0.9519698763650407, 0.968297008471002, 0.9785587609063515, 0.996699002484895, 0.9399427813546489, 0.976461082682467, 0.9720512833542525, 0.991054378039423, 0.9904608489295641, 0.9781796746769382, 0.9647449806645697, 0.9677807194507824, 0.9909009141771136, 0.9828892875109889, 0.9581958341167444, 0.9798118797290499, 0.9618545752049149, 0.9930849793433576, 0.994226303299473, 0.994273053880044, 0.9912901635762161, 0.9968544989811422, 0.9933116180274304, 0.9935951704617636, 0.9950667974327834, 0.9905980517203705, 0.9829096138503677, 0.979698052228529, 0.9743735676282719, 0.9903175482369442, 0.9813261920127649, 0.9943787508448135, 0.9312990055338459, 0.9780048681582812, 0.9884759818892316, 0.8546595592233306]\n",
      "tensor([0.9489, 0.9319, 0.9716, 0.9520, 0.9683, 0.9786, 0.9967, 0.9399, 0.9765,\n",
      "        0.9721, 0.9911, 0.9905, 0.9782, 0.9647, 0.9678, 0.9909, 0.9829, 0.9582,\n",
      "        0.9798, 0.9619, 0.9931, 0.9942, 0.9943, 0.9913, 0.9969, 0.9933, 0.9936,\n",
      "        0.9951, 0.9906, 0.9829, 0.9797, 0.9744, 0.9903, 0.9813, 0.9944, 0.9313,\n",
      "        0.9780, 0.9885, 0.8547], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_weights(train_y, method=0):\n",
    "    _cnt = Counter(train_y).items()\n",
    "    _cnt = [(int(e[0]), e[1]) for e in _cnt]\n",
    "    _cnt = sorted(_cnt, key=itemgetter(0))\n",
    "    nSamples = [e[1] for e in _cnt]\n",
    "    total = sum(nSamples)\n",
    "    print(total)\n",
    "    print(nSamples)\n",
    "    \n",
    "    device = get_device()\n",
    "\n",
    "    if method == 0:\n",
    "        weights = None\n",
    "    elif method == 1:\n",
    "        weights = [1 - (x / total) for x in nSamples]\n",
    "        print(weights)\n",
    "    elif method == 2:\n",
    "        percentage = [ x / total for x in nSamples]\n",
    "        print(percentage)\n",
    "        weights = []\n",
    "        for p in percentage:\n",
    "            if p > 0.1:\n",
    "                weights.append(0.75)\n",
    "            elif p < 0.01:\n",
    "                weights.append(1.25)\n",
    "            else:\n",
    "                weights.append(1)\n",
    "    weights = torch.FloatTensor(weights).to(device)\n",
    "    return weights\n",
    "\n",
    "method = 1\n",
    "weights = get_weights(train_y, method=method)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbCfclUIgMTX"
   },
   "source": [
    "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SY7X0lUgb50"
   },
   "source": [
    "Cleanup the unneeded variables to save memory.<br>\n",
    "\n",
    "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of testing data: (451552, 546)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8192\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = TIMITDataset(train_x, train_y)\n",
    "val_set = TIMITDataset(val_x, val_y)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "import gc\n",
    "\n",
    "del train, train_label, train_x, train_y, val_x, val_y\n",
    "gc.collect()\n",
    "\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "test_first = test[:, :-39*SKIP_FRAME]\n",
    "# test_mid = test[:, 39*4:-39*4]\n",
    "test_last = test[:, 39*SKIP_FRAME:]\n",
    "test = np.c_[test_first, test_last]\n",
    "del test_first, test_last\n",
    "\n",
    "print('Size of testing data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqKNvNZwe3V"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYr1ng5fh9pA"
   },
   "source": [
    "Define model architecture, you are encouraged to change and experiment with the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mish activation loaded...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Mish activation loaded...\")\n",
    "    def forward(self,x):\n",
    "        x = x * (torch.tanh(F.softplus(x)))\n",
    "        return x\n",
    "\n",
    "mish = Mish()\n",
    "x = torch.linspace(-10,10,1000)\n",
    "y = mish(x)\n",
    " \n",
    "plt.plot(x,y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lbZrwT6Ny0XL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        input_shape = 39*(11-SKIP_FRAME)*2\n",
    "        Size1 = 1024\n",
    "        Size2 = 1024\n",
    "        Size3 = 1024\n",
    "        Size4 = 1024\n",
    "\n",
    "        prob = 0.5\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(p=0.05),\n",
    "            \n",
    "            nn.Linear(input_shape, Size1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(Size1),\n",
    "            nn.Dropout(p=prob),\n",
    "            \n",
    "            nn.Linear(Size1, Size2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(Size2),\n",
    "            nn.Dropout(p=prob),\n",
    "            \n",
    "            nn.Linear(Size2, Size3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(Size3),\n",
    "            nn.Dropout(p=prob),\n",
    "            \n",
    "            nn.Linear(Size3, Size4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(Size4),\n",
    "            nn.Dropout(p=prob),\n",
    "            \n",
    "            nn.Linear(Size4, 39)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEX-yjHjhGuH"
   },
   "source": [
    "## Fix random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "88xPiUnm0tAd"
   },
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbBcBXkSp6RA"
   },
   "source": [
    "## Training Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 1126,\n",
    "    'num_class': 39,               # number of class\n",
    "    'num_epoch': 2048,             # number of training epoch\n",
    "    'Lookahead': False,             # Using Lookahead\n",
    "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
    "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "        'lr': 1e-3,                 # learning rate \n",
    "        'weight_decay' : 1e-4,          # weight_decay\n",
    "        # 'weight_decay' : 0,          # weight_decay for PReLU\n",
    "        # 'momentum': 0.9,               # momentum for SGD\n",
    "        # 'nesterov': True,            # nesterov for SGD\n",
    "    },\n",
    "    'EARLY_STOP' : 128,              # early stop setting ( = lr_param * 4)\n",
    "    'lr_scheduler': 'ReduceLROnPlateau', # learning rate scheduler\n",
    "#    'lr_scheduler': 'CosineAnnealingLR',\n",
    "    'lr_scheduler_paras': {\n",
    "        'patience' : 16,                # patience for ReduceLROnPlateau\n",
    "        'factor': 0.5,                 # Reduction factor for ReduceLROnPlateau\n",
    "#         'T_max': 64,                # T_max for CosineAnnealingLR\n",
    "    },\n",
    "    'loss_ratio' : 1. ,             # save the model for best accuracy, range: [0-1] 1: valid set, 0: train set\n",
    "    'weights': weights,             # weights for Corss Entropy Loss\n",
    "    'ensemble': True,               # Ensemble when training\n",
    "    'ensemble_thres': 0.768,         # thresold for ensenble prediction (0.759 for cosine, 0.76x for Reduce)\n",
    "    'post_process': True,          # Post processing when ensemble\n",
    "    'final_process': True,          # Post processing after ensemble\n",
    "    'LabelSmoothingLoss': False        # Label Smoothing Loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_criterion(early_stop_cnt, curr_lr, acc, thres = 0.7725):\n",
    "    flag = True\n",
    "    for a in acc:\n",
    "        if a < thres:\n",
    "            flag = False\n",
    "            break\n",
    "        \n",
    "    if flag:\n",
    "        print('acc > ', thres)\n",
    "        return True\n",
    "    elif early_stop_cnt > config['EARLY_STOP']:\n",
    "        print('early_stop_cnt > ', config['EARLY_STOP'])\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_criterion(acc, thres = 0.76):\n",
    "    if acc[0] > thres and acc[1] > thres:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "def get_date_time():\n",
    "    # datetime object containing current date and time\n",
    "    # dd/mm/YY H:M:S\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    print('Now:', dt_string)\n",
    "    \n",
    "    today = dt_string[5:10].replace('/', '')\n",
    "    print(\"Today's date:\", today)\n",
    "    \n",
    "    CURR_TIME = dt_string[-8:-3].replace(':', '')\n",
    "    print(\"current time:\", CURR_TIME)\n",
    "    \n",
    "    return today, CURR_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "model = Classifier().to(device)\n",
    "## Initializer\n",
    "# initializer = XavierInitializer()\n",
    "# model.apply(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfUECMFCn5VG"
   },
   "source": [
    "Create a testing dataset, and load model from the saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing dataset\n",
    "test_set = TIMITDataset(test, None)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "PREDICT = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(predict):\n",
    "    N = len(predict)\n",
    "    case_ABA_cnt = 0\n",
    "    case_ABC_cnt = 0\n",
    "    continuous_cnt = {}\n",
    "    temp_cnt = 1\n",
    "    for i in range(1, N-1):\n",
    "        if predict[i] == predict[i-1]:\n",
    "            temp_cnt += 1\n",
    "        else:\n",
    "            if temp_cnt not in continuous_cnt:\n",
    "                continuous_cnt[temp_cnt] = 1\n",
    "            else:\n",
    "                continuous_cnt[temp_cnt] += 1\n",
    "            temp_cnt = 1\n",
    "            \n",
    "        if predict[i] != predict[i-1] and predict[i] != predict[i+1]:\n",
    "            if predict[i-1] == predict[i+1]:\n",
    "                predict[i] = predict[i-1]\n",
    "                case_ABA_cnt += 1\n",
    "            else:\n",
    "                case_ABC_cnt += 1\n",
    "                if abs(predict[i] - predict[i-1]) <= abs(predict[i] - predict[i+1]):\n",
    "                    predict[i] = predict[i-1]\n",
    "                else:\n",
    "                    predict[i] = predict[i+1]\n",
    "                 \n",
    "    print('total:', N)\n",
    "    print('case_ABA_cnt:', case_ABA_cnt)\n",
    "    print('case_ABC_cnt:', case_ABC_cnt)\n",
    "    return continuous_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "940TtCCdoYd0"
   },
   "source": [
    "## Make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, test_loader, post_process = True):\n",
    "    predict = []\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs = data\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "\n",
    "            for y in test_pred.cpu().numpy():\n",
    "                predict.append(y)\n",
    "    if post_process:\n",
    "        post_processing(predict)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRYciXZvPbYh"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdMWsBs7zzNs",
    "outputId": "c5ed561e-610d-4a35-d936-fd97adf342a0"
   },
   "outputs": [],
   "source": [
    "from torchtoolbox.nn import LabelSmoothingLoss\n",
    "from torchtoolbox.optimizer import Lookahead\n",
    "from torchtoolbox.nn.init import XavierInitializer\n",
    "\n",
    "def Train(config, train_loader, val_loader, val_set, test_loader, model=None):\n",
    "    # create model, define a loss function, and optimizer\n",
    "    if not model:\n",
    "        device = get_device()\n",
    "        model = Classifier().to(device)\n",
    "        # # Initializer\n",
    "        # initializer = XavierInitializer()\n",
    "        # model.apply(initializer)\n",
    "        \n",
    "    print(model)\n",
    "    print(config)\n",
    "    today, CURR_TIME = get_date_time()\n",
    "    \n",
    "    # get device \n",
    "    device = get_device()\n",
    "    print(f'DEVICE: {device}')\n",
    "    \n",
    "    # Loss\n",
    "    if config['LabelSmoothingLoss']:\n",
    "        criterion = LabelSmoothingLoss(config['num_class'], smoothing=0.1)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=config['weights']) \n",
    "    \n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])\n",
    "    if config['Lookahead']:\n",
    "        optimizer = Lookahead(optimizer)\n",
    "    \n",
    "    scheduler = getattr(torch.optim.lr_scheduler, config['lr_scheduler'])(optimizer, **config['lr_scheduler_paras']) # learning rate scheduler\n",
    "    \n",
    "    # start training\n",
    "    start = datetime.now()\n",
    "    dt_string = start.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    print('Strat training at:', dt_string)\n",
    "\n",
    "    # the path where checkpoint saved\n",
    "    model_path = './models/model_{}_{}.ckpt'.format(today, CURR_TIME)\n",
    "\n",
    "    early_stop_cnt = 0\n",
    "    ensemble_cnt = 0\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    lr = config['optim_hparas']['lr']\n",
    "    for epoch in range(config['num_epoch']):\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # training\n",
    "        model.train() # set the model to training mode\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs) \n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "            batch_loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "            train_loss += batch_loss.item()\n",
    "        \n",
    "        train_acc /= len(train_set)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # validation\n",
    "        if len(val_set) > 0:\n",
    "            model.eval() # set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    batch_loss = criterion(outputs, labels) \n",
    "                    _, val_pred = torch.max(outputs, 1) \n",
    "\n",
    "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                scheduler.step(val_loss)\n",
    "                # scheduler.step()\n",
    "                \n",
    "                val_acc /= len(val_set)\n",
    "                val_loss /= len(val_loader)\n",
    "                \n",
    "                if epoch % 5 == 0:\n",
    "                    print('[{:04d}/{:04d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                        epoch, \n",
    "                        config['num_epoch'], \n",
    "                        train_acc, \n",
    "                        train_loss, \n",
    "                        val_acc, \n",
    "                        val_loss\n",
    "                    ))\n",
    "                    \n",
    "                curr_lr = optimizer.param_groups[0]['lr']\n",
    "                if curr_lr != lr:\n",
    "                    print('    Current learning rate: {:.8f} | early stop cnt: {}'.format(curr_lr, early_stop_cnt))\n",
    "                    lr = curr_lr\n",
    "                \n",
    "                total_acc = val_acc * config['loss_ratio'] + train_acc * (1 - config['loss_ratio'])\n",
    "                acc_check_list = [val_acc, train_acc]\n",
    "                \n",
    "                # if the model improves, save a checkpoint at this epoch\n",
    "                if total_acc > best_acc:\n",
    "                    best_acc = total_acc\n",
    "                    early_stop_cnt = 0\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print('    Saving model with Train Acc: {:.6f} | val acc: {:.6f} | total acc: {:.6f}'.format(\n",
    "                        train_acc, \n",
    "                        val_acc, \n",
    "                        total_acc\n",
    "                    ))\n",
    "                    print()\n",
    "                    if config['ensemble'] and ensemble_criterion(acc=acc_check_list, thres=config['ensemble_thres']):\n",
    "                        ensemble_cnt += 1\n",
    "                        print('    Ensemble_cnt: {} | Train Acc: {:.6f} | Val Acc: {:.6f}'.format(ensemble_cnt, train_acc, val_acc))\n",
    "                        predict = make_prediction(model, test_loader, post_process=config['post_process'])\n",
    "                        PREDICT.append(predict)\n",
    "                else:\n",
    "                    early_stop_cnt += 1\n",
    "                \n",
    "                if stop_criterion(early_stop_cnt=early_stop_cnt, curr_lr=curr_lr, acc=(train_acc, val_acc)):\n",
    "                    # Stop training when satisfying stop criterion\n",
    "                    print('Early stop at {} epoch'.format(epoch+1))\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            print('[{:04d}/{:04d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "                epoch, config['num_epoch'], train_acc, train_loss\n",
    "            ))\n",
    "\n",
    "    # if not validating, save the last epoch\n",
    "    if len(val_set) == 0:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print('saving model at last epoch')\n",
    "    print('learning rate is', optimizer.param_groups[0]['lr'])\n",
    "    print('Ensemble_cnt: {}'.format(ensemble_cnt))\n",
    "    end = datetime.now()\n",
    "    cost_time = end - start\n",
    "    print('Cost Time:', str(cost_time))\n",
    "    return today, CURR_TIME, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (net): Sequential(\n",
      "    (0): Dropout(p=0.05, inplace=False)\n",
      "    (1): Linear(in_features=546, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Dropout(p=0.5, inplace=False)\n",
      "    (13): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (14): ReLU()\n",
      "    (15): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): Dropout(p=0.5, inplace=False)\n",
      "    (17): Linear(in_features=1024, out_features=39, bias=True)\n",
      "  )\n",
      ")\n",
      "{'seed': 1126, 'num_class': 39, 'num_epoch': 2048, 'Lookahead': False, 'optimizer': 'Adam', 'optim_hparas': {'lr': 0.001, 'weight_decay': 0.0001}, 'EARLY_STOP': 128, 'lr_scheduler': 'ReduceLROnPlateau', 'lr_scheduler_paras': {'patience': 16, 'factor': 0.5}, 'loss_ratio': 1.0, 'weights': tensor([0.9489, 0.9319, 0.9716, 0.9520, 0.9683, 0.9786, 0.9967, 0.9399, 0.9765,\n",
      "        0.9721, 0.9911, 0.9905, 0.9782, 0.9647, 0.9678, 0.9909, 0.9829, 0.9582,\n",
      "        0.9798, 0.9619, 0.9931, 0.9942, 0.9943, 0.9913, 0.9969, 0.9933, 0.9936,\n",
      "        0.9951, 0.9906, 0.9829, 0.9797, 0.9744, 0.9903, 0.9813, 0.9944, 0.9313,\n",
      "        0.9780, 0.9885, 0.8547], device='cuda:0'), 'ensemble': True, 'ensemble_thres': 0.768, 'post_process': True, 'final_process': True, 'LabelSmoothingLoss': False}\n",
      "Now: 2021/04/02 08:45:14\n",
      "Today's date: 0402\n",
      "current time: 0845\n",
      "DEVICE: cuda\n",
      "Strat training at: 2021/04/02 08:45:14\n",
      "[0000/2048] Train Acc: 0.520888 Loss: 1.617745 | Val Acc: 0.637875 loss: 1.164510\n",
      "    Saving model with Train Acc: 0.520888 | val acc: 0.637875 | total acc: 0.637875\n",
      "\n",
      "    Saving model with Train Acc: 0.602851 | val acc: 0.668958 | total acc: 0.668958\n",
      "\n",
      "    Saving model with Train Acc: 0.624555 | val acc: 0.684309 | total acc: 0.684309\n",
      "\n",
      "    Saving model with Train Acc: 0.637790 | val acc: 0.696008 | total acc: 0.696008\n",
      "\n",
      "    Saving model with Train Acc: 0.647516 | val acc: 0.700407 | total acc: 0.700407\n",
      "\n",
      "[0005/2048] Train Acc: 0.654525 Loss: 1.093989 | Val Acc: 0.709859 loss: 0.897073\n",
      "    Saving model with Train Acc: 0.654525 | val acc: 0.709859 | total acc: 0.709859\n",
      "\n",
      "    Saving model with Train Acc: 0.661220 | val acc: 0.715481 | total acc: 0.715481\n",
      "\n",
      "    Saving model with Train Acc: 0.665350 | val acc: 0.718627 | total acc: 0.718627\n",
      "\n",
      "    Saving model with Train Acc: 0.669137 | val acc: 0.721071 | total acc: 0.721071\n",
      "\n",
      "    Saving model with Train Acc: 0.672694 | val acc: 0.724807 | total acc: 0.724807\n",
      "\n",
      "[0010/2048] Train Acc: 0.675645 Loss: 1.016308 | Val Acc: 0.728351 loss: 0.828292\n",
      "    Saving model with Train Acc: 0.675645 | val acc: 0.728351 | total acc: 0.728351\n",
      "\n",
      "    Saving model with Train Acc: 0.677263 | val acc: 0.731352 | total acc: 0.731352\n",
      "\n",
      "    Saving model with Train Acc: 0.679900 | val acc: 0.732417 | total acc: 0.732417\n",
      "\n",
      "    Saving model with Train Acc: 0.681900 | val acc: 0.735230 | total acc: 0.735230\n",
      "\n",
      "    Saving model with Train Acc: 0.683473 | val acc: 0.736884 | total acc: 0.736884\n",
      "\n",
      "[0015/2048] Train Acc: 0.685067 Loss: 0.982068 | Val Acc: 0.736868 loss: 0.797268\n",
      "    Saving model with Train Acc: 0.686389 | val acc: 0.739324 | total acc: 0.739324\n",
      "\n",
      "    Saving model with Train Acc: 0.687645 | val acc: 0.740116 | total acc: 0.740116\n",
      "\n",
      "    Saving model with Train Acc: 0.688328 | val acc: 0.742925 | total acc: 0.742925\n",
      "\n",
      "[0020/2048] Train Acc: 0.690669 Loss: 0.960674 | Val Acc: 0.743844 loss: 0.773849\n",
      "    Saving model with Train Acc: 0.690669 | val acc: 0.743844 | total acc: 0.743844\n",
      "\n",
      "    Saving model with Train Acc: 0.692802 | val acc: 0.745381 | total acc: 0.745381\n",
      "\n",
      "    Saving model with Train Acc: 0.693356 | val acc: 0.747568 | total acc: 0.747568\n",
      "\n",
      "[0025/2048] Train Acc: 0.693521 Loss: 0.949339 | Val Acc: 0.746999 loss: 0.760871\n",
      "    Saving model with Train Acc: 0.694046 | val acc: 0.748731 | total acc: 0.748731\n",
      "\n",
      "    Saving model with Train Acc: 0.695558 | val acc: 0.749418 | total acc: 0.749418\n",
      "\n",
      "[0030/2048] Train Acc: 0.696320 Loss: 0.940423 | Val Acc: 0.749121 loss: 0.750305\n",
      "    Saving model with Train Acc: 0.696624 | val acc: 0.749914 | total acc: 0.749914\n",
      "\n",
      "    Saving model with Train Acc: 0.697644 | val acc: 0.749995 | total acc: 0.749995\n",
      "\n",
      "    Saving model with Train Acc: 0.697149 | val acc: 0.750971 | total acc: 0.750971\n",
      "\n",
      "[0035/2048] Train Acc: 0.698729 Loss: 0.932359 | Val Acc: 0.751613 loss: 0.743622\n",
      "    Saving model with Train Acc: 0.698729 | val acc: 0.751613 | total acc: 0.751613\n",
      "\n",
      "    Saving model with Train Acc: 0.699309 | val acc: 0.752048 | total acc: 0.752048\n",
      "\n",
      "    Saving model with Train Acc: 0.698985 | val acc: 0.752971 | total acc: 0.752971\n",
      "\n",
      "[0040/2048] Train Acc: 0.699873 Loss: 0.926555 | Val Acc: 0.752759 loss: 0.737986\n",
      "    Saving model with Train Acc: 0.700122 | val acc: 0.754686 | total acc: 0.754686\n",
      "\n",
      "    Saving model with Train Acc: 0.700230 | val acc: 0.755414 | total acc: 0.755414\n",
      "\n",
      "[0045/2048] Train Acc: 0.701011 Loss: 0.922846 | Val Acc: 0.754877 loss: 0.733069\n",
      "    Saving model with Train Acc: 0.701127 | val acc: 0.756182 | total acc: 0.756182\n",
      "\n",
      "[0050/2048] Train Acc: 0.701677 Loss: 0.919665 | Val Acc: 0.756231 loss: 0.729088\n",
      "    Saving model with Train Acc: 0.701677 | val acc: 0.756231 | total acc: 0.756231\n",
      "\n",
      "    Saving model with Train Acc: 0.702207 | val acc: 0.756251 | total acc: 0.756251\n",
      "\n",
      "    Saving model with Train Acc: 0.702368 | val acc: 0.757284 | total acc: 0.757284\n",
      "\n",
      "    Saving model with Train Acc: 0.703444 | val acc: 0.758016 | total acc: 0.758016\n",
      "\n",
      "[0055/2048] Train Acc: 0.703025 Loss: 0.914491 | Val Acc: 0.756105 loss: 0.728514\n",
      "[0060/2048] Train Acc: 0.703685 Loss: 0.911754 | Val Acc: 0.757398 loss: 0.724294\n",
      "    Saving model with Train Acc: 0.703769 | val acc: 0.758670 | total acc: 0.758670\n",
      "\n",
      "[0065/2048] Train Acc: 0.704063 Loss: 0.910753 | Val Acc: 0.757170 loss: 0.723174\n",
      "    Saving model with Train Acc: 0.705001 | val acc: 0.759426 | total acc: 0.759426\n",
      "\n",
      "[0070/2048] Train Acc: 0.705286 Loss: 0.908807 | Val Acc: 0.759353 loss: 0.718607\n",
      "    Saving model with Train Acc: 0.705211 | val acc: 0.759792 | total acc: 0.759792\n",
      "\n",
      "    Saving model with Train Acc: 0.705383 | val acc: 0.760447 | total acc: 0.760447\n",
      "\n",
      "[0075/2048] Train Acc: 0.704905 Loss: 0.906957 | Val Acc: 0.760532 loss: 0.715788\n",
      "    Saving model with Train Acc: 0.704905 | val acc: 0.760532 | total acc: 0.760532\n",
      "\n",
      "[0080/2048] Train Acc: 0.706076 Loss: 0.903813 | Val Acc: 0.760597 loss: 0.715119\n",
      "    Saving model with Train Acc: 0.706076 | val acc: 0.760597 | total acc: 0.760597\n",
      "\n",
      "    Saving model with Train Acc: 0.706181 | val acc: 0.760918 | total acc: 0.760918\n",
      "\n",
      "    Saving model with Train Acc: 0.706035 | val acc: 0.761024 | total acc: 0.761024\n",
      "\n",
      "[0085/2048] Train Acc: 0.706296 Loss: 0.903218 | Val Acc: 0.760626 loss: 0.714627\n",
      "    Saving model with Train Acc: 0.706296 | val acc: 0.761162 | total acc: 0.761162\n",
      "\n",
      "    Saving model with Train Acc: 0.706943 | val acc: 0.761256 | total acc: 0.761256\n",
      "\n",
      "    Saving model with Train Acc: 0.706254 | val acc: 0.761902 | total acc: 0.761902\n",
      "\n",
      "[0090/2048] Train Acc: 0.707183 Loss: 0.900934 | Val Acc: 0.760398 loss: 0.713164\n",
      "    Saving model with Train Acc: 0.706458 | val acc: 0.762235 | total acc: 0.762235\n",
      "\n",
      "[0095/2048] Train Acc: 0.706701 Loss: 0.901218 | Val Acc: 0.760963 loss: 0.711756\n",
      "    Saving model with Train Acc: 0.707836 | val acc: 0.762703 | total acc: 0.762703\n",
      "\n",
      "[0100/2048] Train Acc: 0.706835 Loss: 0.901397 | Val Acc: 0.762036 loss: 0.709644\n",
      "[0105/2048] Train Acc: 0.707483 Loss: 0.897895 | Val Acc: 0.760495 loss: 0.712382\n",
      "[0110/2048] Train Acc: 0.707754 Loss: 0.898141 | Val Acc: 0.762805 loss: 0.706176\n",
      "    Saving model with Train Acc: 0.707754 | val acc: 0.762805 | total acc: 0.762805\n",
      "\n",
      "    Saving model with Train Acc: 0.708029 | val acc: 0.763305 | total acc: 0.763305\n",
      "\n",
      "[0115/2048] Train Acc: 0.708169 Loss: 0.896880 | Val Acc: 0.762211 loss: 0.708091\n",
      "[0120/2048] Train Acc: 0.707908 Loss: 0.896965 | Val Acc: 0.762073 loss: 0.707608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0125/2048] Train Acc: 0.708086 Loss: 0.896736 | Val Acc: 0.762874 loss: 0.705362\n",
      "    Saving model with Train Acc: 0.707646 | val acc: 0.763626 | total acc: 0.763626\n",
      "\n",
      "[0130/2048] Train Acc: 0.708701 Loss: 0.894827 | Val Acc: 0.763496 loss: 0.703520\n",
      "[0135/2048] Train Acc: 0.708752 Loss: 0.894674 | Val Acc: 0.761975 loss: 0.705003\n",
      "    Saving model with Train Acc: 0.708974 | val acc: 0.764203 | total acc: 0.764203\n",
      "\n",
      "[0140/2048] Train Acc: 0.708203 Loss: 0.894329 | Val Acc: 0.762922 loss: 0.705251\n",
      "[0145/2048] Train Acc: 0.708834 Loss: 0.893011 | Val Acc: 0.763370 loss: 0.704456\n",
      "[0150/2048] Train Acc: 0.709048 Loss: 0.893618 | Val Acc: 0.763772 loss: 0.701142\n",
      "    Saving model with Train Acc: 0.708695 | val acc: 0.764349 | total acc: 0.764349\n",
      "\n",
      "[0155/2048] Train Acc: 0.709915 Loss: 0.891824 | Val Acc: 0.762101 loss: 0.705507\n",
      "    Saving model with Train Acc: 0.708984 | val acc: 0.765158 | total acc: 0.765158\n",
      "\n",
      "[0160/2048] Train Acc: 0.708727 Loss: 0.893923 | Val Acc: 0.762805 loss: 0.704021\n",
      "[0165/2048] Train Acc: 0.709213 Loss: 0.892284 | Val Acc: 0.763760 loss: 0.700913\n",
      "[0170/2048] Train Acc: 0.709145 Loss: 0.892997 | Val Acc: 0.762906 loss: 0.703438\n",
      "[0175/2048] Train Acc: 0.709198 Loss: 0.891693 | Val Acc: 0.764105 loss: 0.701114\n",
      "[0180/2048] Train Acc: 0.709103 Loss: 0.891986 | Val Acc: 0.764272 loss: 0.701212\n",
      "    Saving model with Train Acc: 0.709590 | val acc: 0.765252 | total acc: 0.765252\n",
      "\n",
      "[0185/2048] Train Acc: 0.710097 Loss: 0.890521 | Val Acc: 0.762918 loss: 0.703361\n",
      "    Saving model with Train Acc: 0.709068 | val acc: 0.765418 | total acc: 0.765418\n",
      "\n",
      "[0190/2048] Train Acc: 0.709492 Loss: 0.892453 | Val Acc: 0.765280 loss: 0.698562\n",
      "[0195/2048] Train Acc: 0.710107 Loss: 0.890686 | Val Acc: 0.765118 loss: 0.700302\n",
      "[0200/2048] Train Acc: 0.709417 Loss: 0.892278 | Val Acc: 0.763817 loss: 0.701334\n",
      "    Saving model with Train Acc: 0.710004 | val acc: 0.765614 | total acc: 0.765614\n",
      "\n",
      "[0205/2048] Train Acc: 0.709499 Loss: 0.890320 | Val Acc: 0.765040 loss: 0.697146\n",
      "[0210/2048] Train Acc: 0.710125 Loss: 0.889668 | Val Acc: 0.765191 loss: 0.695673\n",
      "    Saving model with Train Acc: 0.709650 | val acc: 0.765634 | total acc: 0.765634\n",
      "\n",
      "[0215/2048] Train Acc: 0.710444 Loss: 0.889337 | Val Acc: 0.764569 loss: 0.699267\n",
      "    Saving model with Train Acc: 0.710225 | val acc: 0.765715 | total acc: 0.765715\n",
      "\n",
      "    Saving model with Train Acc: 0.710296 | val acc: 0.766118 | total acc: 0.766118\n",
      "\n",
      "[0220/2048] Train Acc: 0.709576 Loss: 0.890568 | Val Acc: 0.763918 loss: 0.699862\n",
      "[0225/2048] Train Acc: 0.710076 Loss: 0.889470 | Val Acc: 0.765093 loss: 0.697705\n",
      "    Current learning rate: 0.00050000 | early stop cnt: 6\n",
      "    Saving model with Train Acc: 0.717368 | val acc: 0.771894 | total acc: 0.771894\n",
      "\n",
      "    Saving model with Train Acc: 0.721943 | val acc: 0.773663 | total acc: 0.773663\n",
      "\n",
      "    Saving model with Train Acc: 0.722237 | val acc: 0.774435 | total acc: 0.774435\n",
      "\n",
      "[0230/2048] Train Acc: 0.723392 Loss: 0.843412 | Val Acc: 0.773919 loss: 0.666721\n",
      "    Saving model with Train Acc: 0.724202 | val acc: 0.775488 | total acc: 0.775488\n",
      "\n",
      "    Saving model with Train Acc: 0.724296 | val acc: 0.775716 | total acc: 0.775716\n",
      "\n",
      "    Saving model with Train Acc: 0.724561 | val acc: 0.776021 | total acc: 0.776021\n",
      "\n",
      "    Saving model with Train Acc: 0.724670 | val acc: 0.776980 | total acc: 0.776980\n",
      "\n",
      "[0235/2048] Train Acc: 0.724604 Loss: 0.837076 | Val Acc: 0.777415 loss: 0.657014\n",
      "    Saving model with Train Acc: 0.724604 | val acc: 0.777415 | total acc: 0.777415\n",
      "\n",
      "    Saving model with Train Acc: 0.725163 | val acc: 0.777618 | total acc: 0.777618\n",
      "\n",
      "[0240/2048] Train Acc: 0.725656 Loss: 0.834870 | Val Acc: 0.777663 loss: 0.653973\n",
      "    Saving model with Train Acc: 0.725656 | val acc: 0.777663 | total acc: 0.777663\n",
      "\n",
      "    Saving model with Train Acc: 0.725490 | val acc: 0.778305 | total acc: 0.778305\n",
      "\n",
      "[0245/2048] Train Acc: 0.726686 Loss: 0.832122 | Val Acc: 0.779285 loss: 0.649438\n",
      "    Saving model with Train Acc: 0.726686 | val acc: 0.779285 | total acc: 0.779285\n",
      "\n",
      "[0250/2048] Train Acc: 0.726410 Loss: 0.829527 | Val Acc: 0.778395 loss: 0.651153\n",
      "    Saving model with Train Acc: 0.726747 | val acc: 0.779427 | total acc: 0.779427\n",
      "\n",
      "    Saving model with Train Acc: 0.726816 | val acc: 0.779769 | total acc: 0.779769\n",
      "\n",
      "[0255/2048] Train Acc: 0.727292 Loss: 0.828666 | Val Acc: 0.779423 loss: 0.648314\n",
      "[0260/2048] Train Acc: 0.727906 Loss: 0.826600 | Val Acc: 0.780244 loss: 0.646976\n",
      "    Saving model with Train Acc: 0.727906 | val acc: 0.780244 | total acc: 0.780244\n",
      "\n",
      "    Saving model with Train Acc: 0.727276 | val acc: 0.780497 | total acc: 0.780497\n",
      "\n",
      "[0265/2048] Train Acc: 0.727606 Loss: 0.826162 | Val Acc: 0.779683 loss: 0.645444\n",
      "[0270/2048] Train Acc: 0.727869 Loss: 0.825073 | Val Acc: 0.779297 loss: 0.645224\n",
      "    Saving model with Train Acc: 0.727815 | val acc: 0.780964 | total acc: 0.780964\n",
      "\n",
      "[0275/2048] Train Acc: 0.727916 Loss: 0.824890 | Val Acc: 0.780261 loss: 0.646008\n",
      "    Saving model with Train Acc: 0.727541 | val acc: 0.781379 | total acc: 0.781379\n",
      "\n",
      "[0280/2048] Train Acc: 0.727987 Loss: 0.824523 | Val Acc: 0.781362 loss: 0.642954\n",
      "[0285/2048] Train Acc: 0.728063 Loss: 0.824251 | Val Acc: 0.782277 loss: 0.639736\n",
      "    Saving model with Train Acc: 0.728063 | val acc: 0.782277 | total acc: 0.782277\n",
      "\n",
      "[0290/2048] Train Acc: 0.727874 Loss: 0.823790 | Val Acc: 0.780887 loss: 0.642302\n",
      "[0295/2048] Train Acc: 0.728453 Loss: 0.822488 | Val Acc: 0.781745 loss: 0.639632\n",
      "[0300/2048] Train Acc: 0.728686 Loss: 0.822035 | Val Acc: 0.781968 loss: 0.640043\n",
      "    Saving model with Train Acc: 0.728660 | val acc: 0.782350 | total acc: 0.782350\n",
      "\n",
      "[0305/2048] Train Acc: 0.728782 Loss: 0.821356 | Val Acc: 0.780366 loss: 0.643468\n",
      "    Saving model with Train Acc: 0.728930 | val acc: 0.782489 | total acc: 0.782489\n",
      "\n",
      "    Saving model with Train Acc: 0.728854 | val acc: 0.782944 | total acc: 0.782944\n",
      "\n",
      "[0310/2048] Train Acc: 0.729129 Loss: 0.820735 | Val Acc: 0.780736 loss: 0.641832\n",
      "[0315/2048] Train Acc: 0.729266 Loss: 0.820868 | Val Acc: 0.781988 loss: 0.639952\n",
      "    Current learning rate: 0.00025000 | early stop cnt: 8\n",
      "    Saving model with Train Acc: 0.733450 | val acc: 0.785785 | total acc: 0.785785\n",
      "\n",
      "[0320/2048] Train Acc: 0.736084 Loss: 0.795041 | Val Acc: 0.787497 loss: 0.621497\n",
      "    Saving model with Train Acc: 0.736084 | val acc: 0.787497 | total acc: 0.787497\n",
      "\n",
      "    Saving model with Train Acc: 0.737582 | val acc: 0.787903 | total acc: 0.787903\n",
      "\n",
      "    Saving model with Train Acc: 0.737997 | val acc: 0.788078 | total acc: 0.788078\n",
      "\n",
      "    Saving model with Train Acc: 0.738321 | val acc: 0.789273 | total acc: 0.789273\n",
      "\n",
      "[0325/2048] Train Acc: 0.739640 Loss: 0.784820 | Val Acc: 0.789444 loss: 0.614137\n",
      "    Saving model with Train Acc: 0.739640 | val acc: 0.789444 | total acc: 0.789444\n",
      "\n",
      "    Saving model with Train Acc: 0.740062 | val acc: 0.790086 | total acc: 0.790086\n",
      "\n",
      "    Saving model with Train Acc: 0.739866 | val acc: 0.790261 | total acc: 0.790261\n",
      "\n",
      "[0330/2048] Train Acc: 0.740484 Loss: 0.779968 | Val Acc: 0.790387 loss: 0.609355\n",
      "    Saving model with Train Acc: 0.740484 | val acc: 0.790387 | total acc: 0.790387\n",
      "\n",
      "    Saving model with Train Acc: 0.740723 | val acc: 0.790713 | total acc: 0.790713\n",
      "\n",
      "    Saving model with Train Acc: 0.741240 | val acc: 0.791448 | total acc: 0.791448\n",
      "\n",
      "[0335/2048] Train Acc: 0.741044 Loss: 0.778499 | Val Acc: 0.791241 loss: 0.607149\n",
      "    Saving model with Train Acc: 0.740786 | val acc: 0.791591 | total acc: 0.791591\n",
      "\n",
      "    Saving model with Train Acc: 0.741581 | val acc: 0.791778 | total acc: 0.791778\n",
      "\n",
      "[0340/2048] Train Acc: 0.741245 Loss: 0.776910 | Val Acc: 0.792367 loss: 0.605538\n",
      "    Saving model with Train Acc: 0.741245 | val acc: 0.792367 | total acc: 0.792367\n",
      "\n",
      "    Saving model with Train Acc: 0.742286 | val acc: 0.793131 | total acc: 0.793131\n",
      "\n",
      "[0345/2048] Train Acc: 0.742258 Loss: 0.775228 | Val Acc: 0.791774 loss: 0.604787\n",
      "[0350/2048] Train Acc: 0.741660 Loss: 0.776249 | Val Acc: 0.792741 loss: 0.601983\n",
      "[0355/2048] Train Acc: 0.742115 Loss: 0.773710 | Val Acc: 0.792570 loss: 0.602810\n",
      "[0360/2048] Train Acc: 0.742816 Loss: 0.772489 | Val Acc: 0.793009 loss: 0.601771\n",
      "    Saving model with Train Acc: 0.742673 | val acc: 0.793408 | total acc: 0.793408\n",
      "\n",
      "[0365/2048] Train Acc: 0.742282 Loss: 0.773493 | Val Acc: 0.793144 loss: 0.600554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saving model with Train Acc: 0.742356 | val acc: 0.793904 | total acc: 0.793904\n",
      "\n",
      "[0370/2048] Train Acc: 0.742451 Loss: 0.772805 | Val Acc: 0.793221 loss: 0.598793\n",
      "    Saving model with Train Acc: 0.742430 | val acc: 0.794416 | total acc: 0.794416\n",
      "\n",
      "[0375/2048] Train Acc: 0.742227 Loss: 0.773468 | Val Acc: 0.794327 loss: 0.597466\n",
      "[0380/2048] Train Acc: 0.743172 Loss: 0.772369 | Val Acc: 0.793477 loss: 0.598784\n",
      "[0385/2048] Train Acc: 0.742946 Loss: 0.772203 | Val Acc: 0.794253 loss: 0.597562\n",
      "[0390/2048] Train Acc: 0.743037 Loss: 0.770763 | Val Acc: 0.793339 loss: 0.598505\n",
      "    Saving model with Train Acc: 0.742800 | val acc: 0.794794 | total acc: 0.794794\n",
      "\n",
      "[0395/2048] Train Acc: 0.742557 Loss: 0.771548 | Val Acc: 0.794477 loss: 0.596683\n",
      "[0400/2048] Train Acc: 0.743121 Loss: 0.771046 | Val Acc: 0.794859 loss: 0.596329\n",
      "    Saving model with Train Acc: 0.743121 | val acc: 0.794859 | total acc: 0.794859\n",
      "\n",
      "[0405/2048] Train Acc: 0.743598 Loss: 0.769743 | Val Acc: 0.793794 loss: 0.597549\n",
      "    Saving model with Train Acc: 0.742830 | val acc: 0.794985 | total acc: 0.794985\n",
      "\n",
      "[0410/2048] Train Acc: 0.743743 Loss: 0.770109 | Val Acc: 0.795184 loss: 0.596369\n",
      "    Saving model with Train Acc: 0.743743 | val acc: 0.795184 | total acc: 0.795184\n",
      "\n",
      "    Saving model with Train Acc: 0.742990 | val acc: 0.795510 | total acc: 0.795510\n",
      "\n",
      "[0415/2048] Train Acc: 0.742967 Loss: 0.770861 | Val Acc: 0.794355 loss: 0.596759\n",
      "[0420/2048] Train Acc: 0.743323 Loss: 0.770129 | Val Acc: 0.794879 loss: 0.596707\n",
      "[0425/2048] Train Acc: 0.743087 Loss: 0.770590 | Val Acc: 0.794131 loss: 0.596716\n",
      "[0430/2048] Train Acc: 0.743210 Loss: 0.769382 | Val Acc: 0.795115 loss: 0.595674\n",
      "[0435/2048] Train Acc: 0.743462 Loss: 0.770132 | Val Acc: 0.794676 loss: 0.594735\n",
      "[0440/2048] Train Acc: 0.743160 Loss: 0.770214 | Val Acc: 0.794957 loss: 0.596040\n",
      "    Saving model with Train Acc: 0.743609 | val acc: 0.795562 | total acc: 0.795562\n",
      "\n",
      "    Saving model with Train Acc: 0.743412 | val acc: 0.795697 | total acc: 0.795697\n",
      "\n",
      "[0445/2048] Train Acc: 0.743522 Loss: 0.768686 | Val Acc: 0.795314 loss: 0.594880\n",
      "    Saving model with Train Acc: 0.743440 | val acc: 0.796481 | total acc: 0.796481\n",
      "\n",
      "[0450/2048] Train Acc: 0.743576 Loss: 0.768697 | Val Acc: 0.794928 loss: 0.595127\n",
      "[0455/2048] Train Acc: 0.743954 Loss: 0.768578 | Val Acc: 0.795542 loss: 0.593417\n",
      "[0460/2048] Train Acc: 0.743740 Loss: 0.769361 | Val Acc: 0.794798 loss: 0.594787\n",
      "[0465/2048] Train Acc: 0.743619 Loss: 0.769049 | Val Acc: 0.795286 loss: 0.594392\n",
      "[0470/2048] Train Acc: 0.744242 Loss: 0.767573 | Val Acc: 0.794611 loss: 0.594202\n",
      "    Current learning rate: 0.00012500 | early stop cnt: 26\n",
      "[0475/2048] Train Acc: 0.746664 Loss: 0.758373 | Val Acc: 0.797961 loss: 0.587024\n",
      "    Saving model with Train Acc: 0.746664 | val acc: 0.797961 | total acc: 0.797961\n",
      "\n",
      "    Saving model with Train Acc: 0.748172 | val acc: 0.798266 | total acc: 0.798266\n",
      "\n",
      "    Saving model with Train Acc: 0.749214 | val acc: 0.798762 | total acc: 0.798762\n",
      "\n",
      "    Saving model with Train Acc: 0.749212 | val acc: 0.799355 | total acc: 0.799355\n",
      "\n",
      "[0480/2048] Train Acc: 0.750101 Loss: 0.745432 | Val Acc: 0.799933 loss: 0.579927\n",
      "    Saving model with Train Acc: 0.750101 | val acc: 0.799933 | total acc: 0.799933\n",
      "\n",
      "    Saving model with Train Acc: 0.750414 | val acc: 0.800339 | total acc: 0.800339\n",
      "\n",
      "    Saving model with Train Acc: 0.750679 | val acc: 0.800782 | total acc: 0.800782\n",
      "\n",
      "    Saving model with Train Acc: 0.751026 | val acc: 0.800794 | total acc: 0.800794\n",
      "\n",
      "[0485/2048] Train Acc: 0.751309 Loss: 0.742035 | Val Acc: 0.800660 loss: 0.577692\n",
      "    Saving model with Train Acc: 0.751532 | val acc: 0.800924 | total acc: 0.800924\n",
      "\n",
      "    Saving model with Train Acc: 0.751553 | val acc: 0.801591 | total acc: 0.801591\n",
      "\n",
      "[0490/2048] Train Acc: 0.752302 Loss: 0.739148 | Val Acc: 0.801754 loss: 0.574498\n",
      "    Saving model with Train Acc: 0.752302 | val acc: 0.801754 | total acc: 0.801754\n",
      "\n",
      "    Saving model with Train Acc: 0.751956 | val acc: 0.802099 | total acc: 0.802099\n",
      "\n",
      "[0495/2048] Train Acc: 0.752414 Loss: 0.738288 | Val Acc: 0.801412 loss: 0.573861\n",
      "    Saving model with Train Acc: 0.752540 | val acc: 0.802603 | total acc: 0.802603\n",
      "\n",
      "    Saving model with Train Acc: 0.752855 | val acc: 0.802693 | total acc: 0.802693\n",
      "\n",
      "[0500/2048] Train Acc: 0.752742 Loss: 0.737801 | Val Acc: 0.803095 loss: 0.571499\n",
      "    Saving model with Train Acc: 0.752742 | val acc: 0.803095 | total acc: 0.803095\n",
      "\n",
      "[0505/2048] Train Acc: 0.752678 Loss: 0.736058 | Val Acc: 0.802965 loss: 0.570403\n",
      "    Saving model with Train Acc: 0.753220 | val acc: 0.803168 | total acc: 0.803168\n",
      "\n",
      "[0510/2048] Train Acc: 0.753166 Loss: 0.735801 | Val Acc: 0.802762 loss: 0.570699\n",
      "[0515/2048] Train Acc: 0.753456 Loss: 0.735059 | Val Acc: 0.803018 loss: 0.568773\n",
      "    Saving model with Train Acc: 0.753750 | val acc: 0.803510 | total acc: 0.803510\n",
      "\n",
      "[0520/2048] Train Acc: 0.753389 Loss: 0.734489 | Val Acc: 0.803392 loss: 0.568145\n",
      "    Saving model with Train Acc: 0.753736 | val acc: 0.803571 | total acc: 0.803571\n",
      "\n",
      "[0525/2048] Train Acc: 0.754133 Loss: 0.733256 | Val Acc: 0.803388 loss: 0.567896\n",
      "    Saving model with Train Acc: 0.753854 | val acc: 0.803823 | total acc: 0.803823\n",
      "\n",
      "    Saving model with Train Acc: 0.753329 | val acc: 0.803925 | total acc: 0.803925\n",
      "\n",
      "    Saving model with Train Acc: 0.753516 | val acc: 0.804103 | total acc: 0.804103\n",
      "\n",
      "[0530/2048] Train Acc: 0.753829 Loss: 0.733937 | Val Acc: 0.803912 loss: 0.567089\n",
      "    Saving model with Train Acc: 0.753515 | val acc: 0.804238 | total acc: 0.804238\n",
      "\n",
      "[0535/2048] Train Acc: 0.753775 Loss: 0.733039 | Val Acc: 0.803750 loss: 0.567079\n",
      "    Saving model with Train Acc: 0.753626 | val acc: 0.804315 | total acc: 0.804315\n",
      "\n",
      "[0540/2048] Train Acc: 0.754261 Loss: 0.732161 | Val Acc: 0.803636 loss: 0.566719\n",
      "    Saving model with Train Acc: 0.754032 | val acc: 0.804587 | total acc: 0.804587\n",
      "\n",
      "[0545/2048] Train Acc: 0.753885 Loss: 0.732830 | Val Acc: 0.804026 loss: 0.565434\n",
      "    Saving model with Train Acc: 0.753970 | val acc: 0.804725 | total acc: 0.804725\n",
      "\n",
      "[0550/2048] Train Acc: 0.753999 Loss: 0.733526 | Val Acc: 0.804234 loss: 0.565310\n",
      "[0555/2048] Train Acc: 0.753965 Loss: 0.731043 | Val Acc: 0.804099 loss: 0.565800\n",
      "[0560/2048] Train Acc: 0.753786 Loss: 0.733444 | Val Acc: 0.804152 loss: 0.565260\n",
      "[0565/2048] Train Acc: 0.753914 Loss: 0.732042 | Val Acc: 0.804831 loss: 0.564075\n",
      "    Saving model with Train Acc: 0.753914 | val acc: 0.804831 | total acc: 0.804831\n",
      "\n",
      "[0570/2048] Train Acc: 0.753970 Loss: 0.731814 | Val Acc: 0.804754 loss: 0.563722\n",
      "[0575/2048] Train Acc: 0.754032 Loss: 0.732064 | Val Acc: 0.804445 loss: 0.563604\n",
      "[0580/2048] Train Acc: 0.753949 Loss: 0.732511 | Val Acc: 0.804579 loss: 0.564168\n",
      "    Saving model with Train Acc: 0.753908 | val acc: 0.805075 | total acc: 0.805075\n",
      "\n",
      "[0585/2048] Train Acc: 0.754223 Loss: 0.732208 | Val Acc: 0.804717 loss: 0.563861\n",
      "[0590/2048] Train Acc: 0.754244 Loss: 0.731462 | Val Acc: 0.805673 loss: 0.562453\n",
      "    Saving model with Train Acc: 0.754244 | val acc: 0.805673 | total acc: 0.805673\n",
      "\n",
      "[0595/2048] Train Acc: 0.753507 Loss: 0.732090 | Val Acc: 0.805104 loss: 0.563148\n",
      "[0600/2048] Train Acc: 0.754738 Loss: 0.730413 | Val Acc: 0.804973 loss: 0.562565\n",
      "[0605/2048] Train Acc: 0.754532 Loss: 0.731189 | Val Acc: 0.804876 loss: 0.562700\n",
      "[0610/2048] Train Acc: 0.754454 Loss: 0.731251 | Val Acc: 0.805486 loss: 0.562468\n",
      "[0615/2048] Train Acc: 0.754388 Loss: 0.731375 | Val Acc: 0.804530 loss: 0.563281\n",
      "[0620/2048] Train Acc: 0.754056 Loss: 0.732020 | Val Acc: 0.804311 loss: 0.563522\n",
      "[0625/2048] Train Acc: 0.753869 Loss: 0.731956 | Val Acc: 0.804656 loss: 0.562597\n",
      "[0630/2048] Train Acc: 0.754462 Loss: 0.730187 | Val Acc: 0.804823 loss: 0.561976\n",
      "    Current learning rate: 0.00006250 | early stop cnt: 39\n",
      "    Saving model with Train Acc: 0.756204 | val acc: 0.805823 | total acc: 0.805823\n",
      "\n",
      "    Saving model with Train Acc: 0.757143 | val acc: 0.806502 | total acc: 0.806502\n",
      "\n",
      "    Saving model with Train Acc: 0.756922 | val acc: 0.806721 | total acc: 0.806721\n",
      "\n",
      "    Saving model with Train Acc: 0.757855 | val acc: 0.807010 | total acc: 0.807010\n",
      "\n",
      "[0635/2048] Train Acc: 0.758224 Loss: 0.717696 | Val Acc: 0.806803 loss: 0.555624\n",
      "    Saving model with Train Acc: 0.758491 | val acc: 0.807522 | total acc: 0.807522\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saving model with Train Acc: 0.758556 | val acc: 0.807563 | total acc: 0.807563\n",
      "\n",
      "    Saving model with Train Acc: 0.758582 | val acc: 0.807762 | total acc: 0.807762\n",
      "\n",
      "[0640/2048] Train Acc: 0.758879 Loss: 0.715784 | Val Acc: 0.808278 loss: 0.553091\n",
      "    Saving model with Train Acc: 0.758879 | val acc: 0.808278 | total acc: 0.808278\n",
      "\n",
      "    Saving model with Train Acc: 0.759246 | val acc: 0.809189 | total acc: 0.809189\n",
      "\n",
      "[0645/2048] Train Acc: 0.758959 Loss: 0.715440 | Val Acc: 0.808083 loss: 0.552572\n",
      "[0650/2048] Train Acc: 0.759583 Loss: 0.713140 | Val Acc: 0.808864 loss: 0.550608\n",
      "[0655/2048] Train Acc: 0.759495 Loss: 0.712537 | Val Acc: 0.808957 loss: 0.549993\n",
      "    Saving model with Train Acc: 0.759568 | val acc: 0.809246 | total acc: 0.809246\n",
      "\n",
      "    Saving model with Train Acc: 0.760052 | val acc: 0.809400 | total acc: 0.809400\n",
      "\n",
      "    Saving model with Train Acc: 0.759827 | val acc: 0.809787 | total acc: 0.809787\n",
      "\n",
      "[0660/2048] Train Acc: 0.759697 Loss: 0.712634 | Val Acc: 0.809466 loss: 0.549674\n",
      "    Saving model with Train Acc: 0.760093 | val acc: 0.810120 | total acc: 0.810120\n",
      "\n",
      "[0665/2048] Train Acc: 0.759784 Loss: 0.711102 | Val Acc: 0.809535 loss: 0.548519\n",
      "[0670/2048] Train Acc: 0.760236 Loss: 0.710750 | Val Acc: 0.809457 loss: 0.548771\n",
      "[0675/2048] Train Acc: 0.760261 Loss: 0.709888 | Val Acc: 0.809758 loss: 0.547475\n",
      "    Saving model with Train Acc: 0.760945 | val acc: 0.810226 | total acc: 0.810226\n",
      "\n",
      "[0680/2048] Train Acc: 0.761223 Loss: 0.709149 | Val Acc: 0.809880 loss: 0.547518\n",
      "[0685/2048] Train Acc: 0.761320 Loss: 0.708450 | Val Acc: 0.809909 loss: 0.546391\n",
      "    Saving model with Train Acc: 0.761199 | val acc: 0.810429 | total acc: 0.810429\n",
      "\n",
      "[0690/2048] Train Acc: 0.761373 Loss: 0.707761 | Val Acc: 0.809783 loss: 0.546354\n",
      "[0695/2048] Train Acc: 0.760752 Loss: 0.708395 | Val Acc: 0.810600 loss: 0.544834\n",
      "    Saving model with Train Acc: 0.760752 | val acc: 0.810600 | total acc: 0.810600\n",
      "\n",
      "    Saving model with Train Acc: 0.761431 | val acc: 0.810608 | total acc: 0.810608\n",
      "\n",
      "    Saving model with Train Acc: 0.761732 | val acc: 0.810673 | total acc: 0.810673\n",
      "\n",
      "[0700/2048] Train Acc: 0.761175 Loss: 0.707660 | Val Acc: 0.810535 loss: 0.545266\n",
      "    Saving model with Train Acc: 0.761532 | val acc: 0.810726 | total acc: 0.810726\n",
      "\n",
      "[0705/2048] Train Acc: 0.760966 Loss: 0.706871 | Val Acc: 0.810730 loss: 0.545439\n",
      "    Saving model with Train Acc: 0.760966 | val acc: 0.810730 | total acc: 0.810730\n",
      "\n",
      "[0710/2048] Train Acc: 0.761686 Loss: 0.706936 | Val Acc: 0.810510 loss: 0.545148\n",
      "    Saving model with Train Acc: 0.761100 | val acc: 0.811392 | total acc: 0.811392\n",
      "\n",
      "[0715/2048] Train Acc: 0.760722 Loss: 0.707367 | Val Acc: 0.810860 loss: 0.544683\n",
      "[0720/2048] Train Acc: 0.761327 Loss: 0.706576 | Val Acc: 0.810527 loss: 0.544401\n",
      "[0725/2048] Train Acc: 0.761073 Loss: 0.707264 | Val Acc: 0.810681 loss: 0.544655\n",
      "    Current learning rate: 0.00003125 | early stop cnt: 16\n",
      "[0730/2048] Train Acc: 0.762692 Loss: 0.701742 | Val Acc: 0.811433 loss: 0.542394\n",
      "    Saving model with Train Acc: 0.762692 | val acc: 0.811433 | total acc: 0.811433\n",
      "\n",
      "    Saving model with Train Acc: 0.762818 | val acc: 0.811779 | total acc: 0.811779\n",
      "\n",
      "    Saving model with Train Acc: 0.763292 | val acc: 0.811864 | total acc: 0.811864\n",
      "\n",
      "    Saving model with Train Acc: 0.763734 | val acc: 0.811974 | total acc: 0.811974\n",
      "\n",
      "[0735/2048] Train Acc: 0.763385 Loss: 0.699248 | Val Acc: 0.812271 loss: 0.540469\n",
      "    Saving model with Train Acc: 0.763385 | val acc: 0.812271 | total acc: 0.812271\n",
      "\n",
      "[0740/2048] Train Acc: 0.764215 Loss: 0.697667 | Val Acc: 0.811941 loss: 0.539481\n",
      "[0745/2048] Train Acc: 0.764122 Loss: 0.697590 | Val Acc: 0.812323 loss: 0.538451\n",
      "    Saving model with Train Acc: 0.764122 | val acc: 0.812323 | total acc: 0.812323\n",
      "\n",
      "    Saving model with Train Acc: 0.764600 | val acc: 0.812449 | total acc: 0.812449\n",
      "\n",
      "    Saving model with Train Acc: 0.764628 | val acc: 0.812523 | total acc: 0.812523\n",
      "\n",
      "[0750/2048] Train Acc: 0.764352 Loss: 0.697823 | Val Acc: 0.812470 loss: 0.538479\n",
      "    Saving model with Train Acc: 0.764117 | val acc: 0.812754 | total acc: 0.812754\n",
      "\n",
      "[0755/2048] Train Acc: 0.764813 Loss: 0.695301 | Val Acc: 0.812701 loss: 0.537922\n",
      "    Saving model with Train Acc: 0.764914 | val acc: 0.813019 | total acc: 0.813019\n",
      "\n",
      "    Saving model with Train Acc: 0.764434 | val acc: 0.813104 | total acc: 0.813104\n",
      "\n",
      "[0760/2048] Train Acc: 0.764901 Loss: 0.695184 | Val Acc: 0.812584 loss: 0.537387\n",
      "[0765/2048] Train Acc: 0.765087 Loss: 0.694358 | Val Acc: 0.812880 loss: 0.536462\n",
      "    Saving model with Train Acc: 0.765044 | val acc: 0.813246 | total acc: 0.813246\n",
      "\n",
      "[0770/2048] Train Acc: 0.765281 Loss: 0.693792 | Val Acc: 0.812998 loss: 0.536084\n",
      "[0775/2048] Train Acc: 0.764876 Loss: 0.695303 | Val Acc: 0.812884 loss: 0.535602\n",
      "    Saving model with Train Acc: 0.765235 | val acc: 0.813421 | total acc: 0.813421\n",
      "\n",
      "[0780/2048] Train Acc: 0.764888 Loss: 0.694220 | Val Acc: 0.813515 loss: 0.534712\n",
      "    Saving model with Train Acc: 0.764888 | val acc: 0.813515 | total acc: 0.813515\n",
      "\n",
      "    Saving model with Train Acc: 0.765128 | val acc: 0.813539 | total acc: 0.813539\n",
      "\n",
      "[0785/2048] Train Acc: 0.765504 Loss: 0.692817 | Val Acc: 0.813380 loss: 0.534796\n",
      "    Saving model with Train Acc: 0.765445 | val acc: 0.813673 | total acc: 0.813673\n",
      "\n",
      "    Saving model with Train Acc: 0.764952 | val acc: 0.813950 | total acc: 0.813950\n",
      "\n",
      "[0790/2048] Train Acc: 0.765088 Loss: 0.693631 | Val Acc: 0.813588 loss: 0.534755\n",
      "[0795/2048] Train Acc: 0.765628 Loss: 0.691731 | Val Acc: 0.813702 loss: 0.534156\n",
      "    Saving model with Train Acc: 0.764714 | val acc: 0.813982 | total acc: 0.813982\n",
      "\n",
      "[0800/2048] Train Acc: 0.765198 Loss: 0.692713 | Val Acc: 0.813872 loss: 0.534284\n",
      "    Saving model with Train Acc: 0.765524 | val acc: 0.814193 | total acc: 0.814193\n",
      "\n",
      "    Saving model with Train Acc: 0.765178 | val acc: 0.814230 | total acc: 0.814230\n",
      "\n",
      "[0805/2048] Train Acc: 0.765337 Loss: 0.692431 | Val Acc: 0.813921 loss: 0.533897\n",
      "    Saving model with Train Acc: 0.765128 | val acc: 0.814344 | total acc: 0.814344\n",
      "\n",
      "[0810/2048] Train Acc: 0.765137 Loss: 0.693603 | Val Acc: 0.814100 loss: 0.533679\n",
      "[0815/2048] Train Acc: 0.766152 Loss: 0.690945 | Val Acc: 0.814433 loss: 0.532811\n",
      "    Saving model with Train Acc: 0.766152 | val acc: 0.814433 | total acc: 0.814433\n",
      "\n",
      "[0820/2048] Train Acc: 0.766021 Loss: 0.690691 | Val Acc: 0.813616 loss: 0.533347\n",
      "[0825/2048] Train Acc: 0.765899 Loss: 0.691493 | Val Acc: 0.813921 loss: 0.532607\n",
      "[0830/2048] Train Acc: 0.766242 Loss: 0.691095 | Val Acc: 0.814019 loss: 0.532336\n",
      "[0835/2048] Train Acc: 0.765989 Loss: 0.691559 | Val Acc: 0.814417 loss: 0.532469\n",
      "[0840/2048] Train Acc: 0.766106 Loss: 0.690577 | Val Acc: 0.813689 loss: 0.533125\n",
      "    Saving model with Train Acc: 0.766442 | val acc: 0.814649 | total acc: 0.814649\n",
      "\n",
      "[0845/2048] Train Acc: 0.765640 Loss: 0.691580 | Val Acc: 0.814031 loss: 0.532234\n",
      "    Saving model with Train Acc: 0.765996 | val acc: 0.814665 | total acc: 0.814665\n",
      "\n",
      "[0850/2048] Train Acc: 0.766034 Loss: 0.690286 | Val Acc: 0.814494 loss: 0.531504\n",
      "[0855/2048] Train Acc: 0.766115 Loss: 0.690040 | Val Acc: 0.814543 loss: 0.531545\n",
      "[0860/2048] Train Acc: 0.765845 Loss: 0.691969 | Val Acc: 0.814637 loss: 0.531992\n",
      "    Saving model with Train Acc: 0.766492 | val acc: 0.814811 | total acc: 0.814811\n",
      "\n",
      "[0865/2048] Train Acc: 0.766128 Loss: 0.690256 | Val Acc: 0.814868 loss: 0.531452\n",
      "    Saving model with Train Acc: 0.766128 | val acc: 0.814868 | total acc: 0.814868\n",
      "\n",
      "[0870/2048] Train Acc: 0.765868 Loss: 0.690598 | Val Acc: 0.814852 loss: 0.531703\n",
      "[0875/2048] Train Acc: 0.766147 Loss: 0.690146 | Val Acc: 0.814799 loss: 0.531490\n",
      "    Saving model with Train Acc: 0.766309 | val acc: 0.814974 | total acc: 0.814974\n",
      "\n",
      "[0880/2048] Train Acc: 0.765941 Loss: 0.690669 | Val Acc: 0.814941 loss: 0.531045\n",
      "    Saving model with Train Acc: 0.765855 | val acc: 0.815181 | total acc: 0.815181\n",
      "\n",
      "[0885/2048] Train Acc: 0.766585 Loss: 0.690611 | Val Acc: 0.814787 loss: 0.530740\n",
      "[0890/2048] Train Acc: 0.766086 Loss: 0.688684 | Val Acc: 0.814393 loss: 0.531223\n",
      "[0895/2048] Train Acc: 0.766396 Loss: 0.689179 | Val Acc: 0.814941 loss: 0.531041\n",
      "[0900/2048] Train Acc: 0.766206 Loss: 0.689556 | Val Acc: 0.814921 loss: 0.530741\n",
      "[0905/2048] Train Acc: 0.766341 Loss: 0.689539 | Val Acc: 0.815092 loss: 0.530309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0910/2048] Train Acc: 0.765927 Loss: 0.690191 | Val Acc: 0.815027 loss: 0.530053\n",
      "[0915/2048] Train Acc: 0.766076 Loss: 0.689307 | Val Acc: 0.814738 loss: 0.530430\n",
      "[0920/2048] Train Acc: 0.766018 Loss: 0.690105 | Val Acc: 0.814905 loss: 0.530303\n",
      "[0925/2048] Train Acc: 0.766424 Loss: 0.690086 | Val Acc: 0.814771 loss: 0.530520\n",
      "    Current learning rate: 0.00001563 | early stop cnt: 46\n",
      "[0930/2048] Train Acc: 0.766977 Loss: 0.687754 | Val Acc: 0.815303 loss: 0.529542\n",
      "    Saving model with Train Acc: 0.766977 | val acc: 0.815303 | total acc: 0.815303\n",
      "\n",
      "    Saving model with Train Acc: 0.767431 | val acc: 0.815393 | total acc: 0.815393\n",
      "\n",
      "    Saving model with Train Acc: 0.767190 | val acc: 0.815409 | total acc: 0.815409\n",
      "\n",
      "[0935/2048] Train Acc: 0.767340 Loss: 0.687274 | Val Acc: 0.815759 loss: 0.528169\n",
      "    Saving model with Train Acc: 0.767340 | val acc: 0.815759 | total acc: 0.815759\n",
      "\n",
      "[0940/2048] Train Acc: 0.767431 Loss: 0.685269 | Val Acc: 0.815624 loss: 0.528068\n",
      "    Saving model with Train Acc: 0.767870 | val acc: 0.815946 | total acc: 0.815946\n",
      "\n",
      "[0945/2048] Train Acc: 0.767265 Loss: 0.685017 | Val Acc: 0.815799 loss: 0.527567\n",
      "    Saving model with Train Acc: 0.767495 | val acc: 0.816100 | total acc: 0.816100\n",
      "\n",
      "[0950/2048] Train Acc: 0.768401 Loss: 0.684288 | Val Acc: 0.815937 loss: 0.527512\n",
      "    Saving model with Train Acc: 0.767286 | val acc: 0.816173 | total acc: 0.816173\n",
      "\n",
      "[0955/2048] Train Acc: 0.767912 Loss: 0.683367 | Val Acc: 0.816267 loss: 0.526698\n",
      "    Saving model with Train Acc: 0.767912 | val acc: 0.816267 | total acc: 0.816267\n",
      "\n",
      "[0960/2048] Train Acc: 0.767614 Loss: 0.684220 | Val Acc: 0.816007 loss: 0.526908\n",
      "    Saving model with Train Acc: 0.767989 | val acc: 0.816291 | total acc: 0.816291\n",
      "\n",
      "[0965/2048] Train Acc: 0.767828 Loss: 0.684293 | Val Acc: 0.816250 loss: 0.526245\n",
      "    Saving model with Train Acc: 0.767675 | val acc: 0.816352 | total acc: 0.816352\n",
      "\n",
      "[0970/2048] Train Acc: 0.768847 Loss: 0.682741 | Val Acc: 0.816165 loss: 0.526301\n",
      "    Saving model with Train Acc: 0.768297 | val acc: 0.816519 | total acc: 0.816519\n",
      "\n",
      "    Ensemble_cnt: 1 | Train Acc: 0.768297 | Val Acc: 0.816519\n",
      "total: 451552\n",
      "case_ABA_cnt: 2207\n",
      "case_ABC_cnt: 8341\n",
      "    Saving model with Train Acc: 0.768531 | val acc: 0.816563 | total acc: 0.816563\n",
      "\n",
      "    Ensemble_cnt: 2 | Train Acc: 0.768531 | Val Acc: 0.816563\n",
      "total: 451552\n",
      "case_ABA_cnt: 2193\n",
      "case_ABC_cnt: 8316\n",
      "[0975/2048] Train Acc: 0.768358 Loss: 0.683371 | Val Acc: 0.816498 loss: 0.526227\n",
      "    Saving model with Train Acc: 0.768055 | val acc: 0.816637 | total acc: 0.816637\n",
      "\n",
      "    Ensemble_cnt: 3 | Train Acc: 0.768055 | Val Acc: 0.816637\n",
      "total: 451552\n",
      "case_ABA_cnt: 2207\n",
      "case_ABC_cnt: 8353\n",
      "[0980/2048] Train Acc: 0.767623 Loss: 0.683610 | Val Acc: 0.816486 loss: 0.525823\n",
      "[0985/2048] Train Acc: 0.768244 Loss: 0.683273 | Val Acc: 0.816539 loss: 0.525954\n",
      "[0990/2048] Train Acc: 0.768785 Loss: 0.682796 | Val Acc: 0.816498 loss: 0.525539\n",
      "    Saving model with Train Acc: 0.768052 | val acc: 0.816689 | total acc: 0.816689\n",
      "\n",
      "    Ensemble_cnt: 4 | Train Acc: 0.768052 | Val Acc: 0.816689\n",
      "total: 451552\n",
      "case_ABA_cnt: 2196\n",
      "case_ABC_cnt: 8391\n",
      "[0995/2048] Train Acc: 0.768744 Loss: 0.681558 | Val Acc: 0.816844 loss: 0.525012\n",
      "    Saving model with Train Acc: 0.768744 | val acc: 0.816844 | total acc: 0.816844\n",
      "\n",
      "    Ensemble_cnt: 5 | Train Acc: 0.768744 | Val Acc: 0.816844\n",
      "total: 451552\n",
      "case_ABA_cnt: 2232\n",
      "case_ABC_cnt: 8364\n",
      "[1000/2048] Train Acc: 0.768796 Loss: 0.681630 | Val Acc: 0.816685 loss: 0.525032\n",
      "[1005/2048] Train Acc: 0.768365 Loss: 0.681290 | Val Acc: 0.816588 loss: 0.525090\n",
      "    Saving model with Train Acc: 0.768158 | val acc: 0.816872 | total acc: 0.816872\n",
      "\n",
      "    Ensemble_cnt: 6 | Train Acc: 0.768158 | Val Acc: 0.816872\n",
      "total: 451552\n",
      "case_ABA_cnt: 2212\n",
      "case_ABC_cnt: 8281\n",
      "    Saving model with Train Acc: 0.768362 | val acc: 0.816942 | total acc: 0.816942\n",
      "\n",
      "    Ensemble_cnt: 7 | Train Acc: 0.768362 | Val Acc: 0.816942\n",
      "total: 451552\n",
      "case_ABA_cnt: 2229\n",
      "case_ABC_cnt: 8405\n",
      "[1010/2048] Train Acc: 0.768645 Loss: 0.682458 | Val Acc: 0.816840 loss: 0.524430\n",
      "    Saving model with Train Acc: 0.769001 | val acc: 0.817120 | total acc: 0.817120\n",
      "\n",
      "    Ensemble_cnt: 8 | Train Acc: 0.769001 | Val Acc: 0.817120\n",
      "total: 451552\n",
      "case_ABA_cnt: 2185\n",
      "case_ABC_cnt: 8425\n",
      "[1015/2048] Train Acc: 0.768672 Loss: 0.682588 | Val Acc: 0.816937 loss: 0.524501\n",
      "[1020/2048] Train Acc: 0.768603 Loss: 0.681913 | Val Acc: 0.816816 loss: 0.524545\n",
      "[1025/2048] Train Acc: 0.768665 Loss: 0.681509 | Val Acc: 0.816673 loss: 0.524533\n",
      "    Saving model with Train Acc: 0.768437 | val acc: 0.817157 | total acc: 0.817157\n",
      "\n",
      "    Ensemble_cnt: 9 | Train Acc: 0.768437 | Val Acc: 0.817157\n",
      "total: 451552\n",
      "case_ABA_cnt: 2169\n",
      "case_ABC_cnt: 8387\n",
      "[1030/2048] Train Acc: 0.769139 Loss: 0.680427 | Val Acc: 0.816848 loss: 0.524208\n",
      "[1035/2048] Train Acc: 0.768506 Loss: 0.681756 | Val Acc: 0.816820 loss: 0.523980\n",
      "    Saving model with Train Acc: 0.768638 | val acc: 0.817230 | total acc: 0.817230\n",
      "\n",
      "    Ensemble_cnt: 10 | Train Acc: 0.768638 | Val Acc: 0.817230\n",
      "total: 451552\n",
      "case_ABA_cnt: 2181\n",
      "case_ABC_cnt: 8455\n",
      "[1040/2048] Train Acc: 0.768904 Loss: 0.681782 | Val Acc: 0.817214 loss: 0.523745\n",
      "[1045/2048] Train Acc: 0.768651 Loss: 0.680675 | Val Acc: 0.817283 loss: 0.523967\n",
      "    Saving model with Train Acc: 0.768651 | val acc: 0.817283 | total acc: 0.817283\n",
      "\n",
      "    Ensemble_cnt: 11 | Train Acc: 0.768651 | Val Acc: 0.817283\n",
      "total: 451552\n",
      "case_ABA_cnt: 2235\n",
      "case_ABC_cnt: 8421\n",
      "    Saving model with Train Acc: 0.769159 | val acc: 0.817287 | total acc: 0.817287\n",
      "\n",
      "    Ensemble_cnt: 12 | Train Acc: 0.769159 | Val Acc: 0.817287\n",
      "total: 451552\n",
      "case_ABA_cnt: 2212\n",
      "case_ABC_cnt: 8411\n",
      "[1050/2048] Train Acc: 0.769188 Loss: 0.680449 | Val Acc: 0.816909 loss: 0.523955\n",
      "[1055/2048] Train Acc: 0.768667 Loss: 0.682185 | Val Acc: 0.816966 loss: 0.524315\n",
      "    Saving model with Train Acc: 0.768906 | val acc: 0.817381 | total acc: 0.817381\n",
      "\n",
      "    Ensemble_cnt: 13 | Train Acc: 0.768906 | Val Acc: 0.817381\n",
      "total: 451552\n",
      "case_ABA_cnt: 2202\n",
      "case_ABC_cnt: 8413\n",
      "[1060/2048] Train Acc: 0.768911 Loss: 0.682004 | Val Acc: 0.816933 loss: 0.524025\n",
      "[1065/2048] Train Acc: 0.768472 Loss: 0.681528 | Val Acc: 0.816933 loss: 0.524376\n",
      "    Current learning rate: 0.00000781 | early stop cnt: 7\n",
      "[1070/2048] Train Acc: 0.769217 Loss: 0.680045 | Val Acc: 0.817417 loss: 0.523124\n",
      "    Saving model with Train Acc: 0.769217 | val acc: 0.817417 | total acc: 0.817417\n",
      "\n",
      "    Ensemble_cnt: 14 | Train Acc: 0.769217 | Val Acc: 0.817417\n",
      "total: 451552\n",
      "case_ABA_cnt: 2241\n",
      "case_ABC_cnt: 8330\n",
      "    Saving model with Train Acc: 0.768688 | val acc: 0.817429 | total acc: 0.817429\n",
      "\n",
      "    Ensemble_cnt: 15 | Train Acc: 0.768688 | Val Acc: 0.817429\n",
      "total: 451552\n",
      "case_ABA_cnt: 2226\n",
      "case_ABC_cnt: 8404\n",
      "[1075/2048] Train Acc: 0.769515 Loss: 0.679467 | Val Acc: 0.817169 loss: 0.523042\n",
      "    Saving model with Train Acc: 0.769194 | val acc: 0.817482 | total acc: 0.817482\n",
      "\n",
      "    Ensemble_cnt: 16 | Train Acc: 0.769194 | Val Acc: 0.817482\n",
      "total: 451552\n",
      "case_ABA_cnt: 2195\n",
      "case_ABC_cnt: 8362\n",
      "[1080/2048] Train Acc: 0.769143 Loss: 0.680076 | Val Acc: 0.817279 loss: 0.522502\n",
      "    Saving model with Train Acc: 0.769555 | val acc: 0.817527 | total acc: 0.817527\n",
      "\n",
      "    Ensemble_cnt: 17 | Train Acc: 0.769555 | Val Acc: 0.817527\n",
      "total: 451552\n",
      "case_ABA_cnt: 2192\n",
      "case_ABC_cnt: 8345\n",
      "    Saving model with Train Acc: 0.769759 | val acc: 0.817543 | total acc: 0.817543\n",
      "\n",
      "    Ensemble_cnt: 18 | Train Acc: 0.769759 | Val Acc: 0.817543\n",
      "total: 451552\n",
      "case_ABA_cnt: 2183\n",
      "case_ABC_cnt: 8341\n",
      "[1085/2048] Train Acc: 0.769720 Loss: 0.678227 | Val Acc: 0.817592 loss: 0.522229\n",
      "    Saving model with Train Acc: 0.769720 | val acc: 0.817592 | total acc: 0.817592\n",
      "\n",
      "    Ensemble_cnt: 19 | Train Acc: 0.769720 | Val Acc: 0.817592\n",
      "total: 451552\n",
      "case_ABA_cnt: 2191\n",
      "case_ABC_cnt: 8375\n",
      "    Saving model with Train Acc: 0.769450 | val acc: 0.817625 | total acc: 0.817625\n",
      "\n",
      "    Ensemble_cnt: 20 | Train Acc: 0.769450 | Val Acc: 0.817625\n",
      "total: 451552\n",
      "case_ABA_cnt: 2228\n",
      "case_ABC_cnt: 8382\n",
      "[1090/2048] Train Acc: 0.769619 Loss: 0.678453 | Val Acc: 0.817490 loss: 0.522026\n",
      "[1095/2048] Train Acc: 0.769686 Loss: 0.678201 | Val Acc: 0.817637 loss: 0.522238\n",
      "    Saving model with Train Acc: 0.769686 | val acc: 0.817637 | total acc: 0.817637\n",
      "\n",
      "    Ensemble_cnt: 21 | Train Acc: 0.769686 | Val Acc: 0.817637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 451552\n",
      "case_ABA_cnt: 2206\n",
      "case_ABC_cnt: 8389\n",
      "    Saving model with Train Acc: 0.769041 | val acc: 0.817657 | total acc: 0.817657\n",
      "\n",
      "    Ensemble_cnt: 22 | Train Acc: 0.769041 | Val Acc: 0.817657\n",
      "total: 451552\n",
      "case_ABA_cnt: 2197\n",
      "case_ABC_cnt: 8403\n",
      "    Saving model with Train Acc: 0.769948 | val acc: 0.817775 | total acc: 0.817775\n",
      "\n",
      "    Ensemble_cnt: 23 | Train Acc: 0.769948 | Val Acc: 0.817775\n",
      "total: 451552\n",
      "case_ABA_cnt: 2170\n",
      "case_ABC_cnt: 8416\n",
      "[1100/2048] Train Acc: 0.769927 Loss: 0.678192 | Val Acc: 0.817559 loss: 0.522226\n",
      "    Current learning rate: 0.00000391 | early stop cnt: 5\n",
      "    Saving model with Train Acc: 0.769754 | val acc: 0.817901 | total acc: 0.817901\n",
      "\n",
      "    Ensemble_cnt: 24 | Train Acc: 0.769754 | Val Acc: 0.817901\n",
      "total: 451552\n",
      "case_ABA_cnt: 2231\n",
      "case_ABC_cnt: 8364\n",
      "[1105/2048] Train Acc: 0.770075 Loss: 0.678335 | Val Acc: 0.817933 loss: 0.521972\n",
      "    Saving model with Train Acc: 0.770075 | val acc: 0.817933 | total acc: 0.817933\n",
      "\n",
      "    Ensemble_cnt: 25 | Train Acc: 0.770075 | Val Acc: 0.817933\n",
      "total: 451552\n",
      "case_ABA_cnt: 2196\n",
      "case_ABC_cnt: 8359\n",
      "    Saving model with Train Acc: 0.769607 | val acc: 0.817946 | total acc: 0.817946\n",
      "\n",
      "    Ensemble_cnt: 26 | Train Acc: 0.769607 | Val Acc: 0.817946\n",
      "total: 451552\n",
      "case_ABA_cnt: 2227\n",
      "case_ABC_cnt: 8341\n",
      "    Saving model with Train Acc: 0.769871 | val acc: 0.817970 | total acc: 0.817970\n",
      "\n",
      "    Ensemble_cnt: 27 | Train Acc: 0.769871 | Val Acc: 0.817970\n",
      "total: 451552\n",
      "case_ABA_cnt: 2195\n",
      "case_ABC_cnt: 8381\n",
      "[1110/2048] Train Acc: 0.770391 Loss: 0.676110 | Val Acc: 0.817925 loss: 0.521906\n",
      "    Saving model with Train Acc: 0.769328 | val acc: 0.817994 | total acc: 0.817994\n",
      "\n",
      "    Ensemble_cnt: 28 | Train Acc: 0.769328 | Val Acc: 0.817994\n",
      "total: 451552\n",
      "case_ABA_cnt: 2225\n",
      "case_ABC_cnt: 8336\n",
      "    Saving model with Train Acc: 0.770095 | val acc: 0.818035 | total acc: 0.818035\n",
      "\n",
      "    Ensemble_cnt: 29 | Train Acc: 0.770095 | Val Acc: 0.818035\n",
      "total: 451552\n",
      "case_ABA_cnt: 2248\n",
      "case_ABC_cnt: 8327\n",
      "[1115/2048] Train Acc: 0.770144 Loss: 0.676960 | Val Acc: 0.817803 loss: 0.521863\n",
      "[1120/2048] Train Acc: 0.769684 Loss: 0.678824 | Val Acc: 0.817702 loss: 0.521403\n",
      "[1125/2048] Train Acc: 0.770073 Loss: 0.676607 | Val Acc: 0.817893 loss: 0.521315\n",
      "[1130/2048] Train Acc: 0.770581 Loss: 0.677327 | Val Acc: 0.817990 loss: 0.521246\n",
      "    Saving model with Train Acc: 0.769750 | val acc: 0.818072 | total acc: 0.818072\n",
      "\n",
      "    Ensemble_cnt: 30 | Train Acc: 0.769750 | Val Acc: 0.818072\n",
      "total: 451552\n",
      "case_ABA_cnt: 2239\n",
      "case_ABC_cnt: 8353\n",
      "[1135/2048] Train Acc: 0.769894 Loss: 0.676761 | Val Acc: 0.817836 loss: 0.521369\n",
      "    Saving model with Train Acc: 0.770302 | val acc: 0.818088 | total acc: 0.818088\n",
      "\n",
      "    Ensemble_cnt: 31 | Train Acc: 0.770302 | Val Acc: 0.818088\n",
      "total: 451552\n",
      "case_ABA_cnt: 2250\n",
      "case_ABC_cnt: 8339\n",
      "[1140/2048] Train Acc: 0.770188 Loss: 0.676675 | Val Acc: 0.818141 loss: 0.521135\n",
      "    Saving model with Train Acc: 0.770188 | val acc: 0.818141 | total acc: 0.818141\n",
      "\n",
      "    Ensemble_cnt: 32 | Train Acc: 0.770188 | Val Acc: 0.818141\n",
      "total: 451552\n",
      "case_ABA_cnt: 2218\n",
      "case_ABC_cnt: 8363\n",
      "    Saving model with Train Acc: 0.769816 | val acc: 0.818246 | total acc: 0.818246\n",
      "\n",
      "    Ensemble_cnt: 33 | Train Acc: 0.769816 | Val Acc: 0.818246\n",
      "total: 451552\n",
      "case_ABA_cnt: 2213\n",
      "case_ABC_cnt: 8390\n",
      "[1145/2048] Train Acc: 0.770109 Loss: 0.675924 | Val Acc: 0.818149 loss: 0.520793\n",
      "    Saving model with Train Acc: 0.770333 | val acc: 0.818279 | total acc: 0.818279\n",
      "\n",
      "    Ensemble_cnt: 34 | Train Acc: 0.770333 | Val Acc: 0.818279\n",
      "total: 451552\n",
      "case_ABA_cnt: 2242\n",
      "case_ABC_cnt: 8361\n",
      "[1150/2048] Train Acc: 0.770525 Loss: 0.676179 | Val Acc: 0.817925 loss: 0.520924\n",
      "[1155/2048] Train Acc: 0.771041 Loss: 0.675703 | Val Acc: 0.817877 loss: 0.521064\n",
      "[1160/2048] Train Acc: 0.770193 Loss: 0.676173 | Val Acc: 0.817950 loss: 0.520887\n",
      "[1165/2048] Train Acc: 0.770454 Loss: 0.676131 | Val Acc: 0.818108 loss: 0.520557\n",
      "    Current learning rate: 0.00000195 | early stop cnt: 19\n",
      "[1170/2048] Train Acc: 0.770692 Loss: 0.676521 | Val Acc: 0.817917 loss: 0.520801\n",
      "    Saving model with Train Acc: 0.770717 | val acc: 0.818316 | total acc: 0.818316\n",
      "\n",
      "    Ensemble_cnt: 35 | Train Acc: 0.770717 | Val Acc: 0.818316\n",
      "total: 451552\n",
      "case_ABA_cnt: 2245\n",
      "case_ABC_cnt: 8308\n",
      "[1175/2048] Train Acc: 0.769894 Loss: 0.676140 | Val Acc: 0.818222 loss: 0.520581\n",
      "    Saving model with Train Acc: 0.770779 | val acc: 0.818336 | total acc: 0.818336\n",
      "\n",
      "    Ensemble_cnt: 36 | Train Acc: 0.770779 | Val Acc: 0.818336\n",
      "total: 451552\n",
      "case_ABA_cnt: 2231\n",
      "case_ABC_cnt: 8345\n",
      "[1180/2048] Train Acc: 0.770620 Loss: 0.675620 | Val Acc: 0.818271 loss: 0.520669\n",
      "[1185/2048] Train Acc: 0.770534 Loss: 0.676671 | Val Acc: 0.818328 loss: 0.520437\n",
      "    Saving model with Train Acc: 0.769813 | val acc: 0.818429 | total acc: 0.818429\n",
      "\n",
      "    Ensemble_cnt: 37 | Train Acc: 0.769813 | Val Acc: 0.818429\n",
      "total: 451552\n",
      "case_ABA_cnt: 2250\n",
      "case_ABC_cnt: 8346\n",
      "[1190/2048] Train Acc: 0.770783 Loss: 0.674716 | Val Acc: 0.818413 loss: 0.520424\n",
      "[1195/2048] Train Acc: 0.770378 Loss: 0.675793 | Val Acc: 0.817994 loss: 0.520521\n",
      "[1200/2048] Train Acc: 0.770242 Loss: 0.676851 | Val Acc: 0.818120 loss: 0.520405\n",
      "[1205/2048] Train Acc: 0.770208 Loss: 0.675870 | Val Acc: 0.818181 loss: 0.520334\n",
      "    Current learning rate: 0.00000098 | early stop cnt: 16\n",
      "[1210/2048] Train Acc: 0.770914 Loss: 0.674605 | Val Acc: 0.818238 loss: 0.520290\n",
      "[1215/2048] Train Acc: 0.770046 Loss: 0.674744 | Val Acc: 0.818181 loss: 0.520316\n",
      "[1220/2048] Train Acc: 0.771016 Loss: 0.675133 | Val Acc: 0.818129 loss: 0.520117\n",
      "    Current learning rate: 0.00000049 | early stop cnt: 33\n",
      "[1225/2048] Train Acc: 0.770946 Loss: 0.673999 | Val Acc: 0.818295 loss: 0.520285\n",
      "[1230/2048] Train Acc: 0.770653 Loss: 0.675069 | Val Acc: 0.818238 loss: 0.520232\n",
      "[1235/2048] Train Acc: 0.770640 Loss: 0.675350 | Val Acc: 0.818251 loss: 0.519966\n",
      "[1240/2048] Train Acc: 0.770452 Loss: 0.675838 | Val Acc: 0.818214 loss: 0.519957\n",
      "[1245/2048] Train Acc: 0.771034 Loss: 0.674931 | Val Acc: 0.818287 loss: 0.520087\n",
      "[1250/2048] Train Acc: 0.770706 Loss: 0.674624 | Val Acc: 0.818226 loss: 0.520199\n",
      "[1255/2048] Train Acc: 0.770522 Loss: 0.676113 | Val Acc: 0.818246 loss: 0.520187\n",
      "[1260/2048] Train Acc: 0.770403 Loss: 0.676871 | Val Acc: 0.818051 loss: 0.520261\n",
      "[1265/2048] Train Acc: 0.770649 Loss: 0.674308 | Val Acc: 0.818088 loss: 0.520217\n",
      "    Current learning rate: 0.00000024 | early stop cnt: 75\n",
      "[1270/2048] Train Acc: 0.769831 Loss: 0.676278 | Val Acc: 0.818401 loss: 0.519947\n",
      "[1275/2048] Train Acc: 0.770923 Loss: 0.674800 | Val Acc: 0.818153 loss: 0.520320\n",
      "[1280/2048] Train Acc: 0.770897 Loss: 0.674637 | Val Acc: 0.818316 loss: 0.519891\n",
      "    Current learning rate: 0.00000012 | early stop cnt: 92\n",
      "[1285/2048] Train Acc: 0.770749 Loss: 0.674593 | Val Acc: 0.818417 loss: 0.519851\n",
      "[1290/2048] Train Acc: 0.770473 Loss: 0.676049 | Val Acc: 0.818275 loss: 0.520029\n",
      "[1295/2048] Train Acc: 0.770577 Loss: 0.674970 | Val Acc: 0.818255 loss: 0.520238\n",
      "    Current learning rate: 0.00000006 | early stop cnt: 109\n",
      "[1300/2048] Train Acc: 0.770349 Loss: 0.674714 | Val Acc: 0.818397 loss: 0.520009\n",
      "[1305/2048] Train Acc: 0.770921 Loss: 0.674051 | Val Acc: 0.818177 loss: 0.520049\n",
      "[1310/2048] Train Acc: 0.770768 Loss: 0.674904 | Val Acc: 0.818169 loss: 0.520399\n",
      "[1315/2048] Train Acc: 0.771166 Loss: 0.673877 | Val Acc: 0.818153 loss: 0.520141\n",
      "    Current learning rate: 0.00000003 | early stop cnt: 126\n",
      "early_stop_cnt >  128\n",
      "Early stop at 1319 epoch\n",
      "learning rate is 3.0517578125e-08\n",
      "Ensemble_cnt: 37\n",
      "Cost Time: 5:03:13.259216\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "same_seeds(config['seed'])\n",
    "today, CURR_TIME, best_acc = Train(config, train_loader, val_loader, val_set, test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "\n",
    "config['num_epoch'] = 256\n",
    "config['optim_hparas']['lr'] = 1e-5 \n",
    "today, CURR_TIME, best_acc = Train(config, train_loader, val_loader, val_set, test_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(PREDICT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hi7jTn3PX-m"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PKjtAScPWtr",
    "outputId": "8c17272b-536a-4692-a95f-a3292766c698"
   },
   "outputs": [],
   "source": [
    "# # Reset PREDICT\n",
    "# PREDICT = []\n",
    "\n",
    "if len(PREDICT) % 2 == 0:\n",
    "    model_path = './models/model_{}_{}.ckpt'.format(today, CURR_TIME)\n",
    "    # model_path = './models/model_0322_1535.ckpt'\n",
    "\n",
    "    # create model and load weights from checkpoint\n",
    "    model = Classifier().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    predict = make_prediction(model, test_loader, post_process = True)\n",
    "    PREDICT.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_PREDICT = PREDICT[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the mode (voting for ensemble results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "84HU5GGjPqR0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "451552\n",
      "(1, 451552)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "#print(len(last_PREDICT))\n",
    "#print(len(last_PREDICT[0]))\n",
    "#mode = stats.mode(last_PREDICT, axis=0)\n",
    "print(len(PREDICT))\n",
    "print(len(PREDICT[0]))\n",
    "mode = stats.mode(PREDICT, axis=0)\n",
    "print(mode[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451552\n"
     ]
    }
   ],
   "source": [
    "N = (mode[0].shape)[1]\n",
    "print(N)\n",
    "result = []\n",
    "for i in range(N):\n",
    "    result.append(mode[0][0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 451552\n",
      "case_ABA_cnt: 2\n",
      "case_ABC_cnt: 14\n"
     ]
    }
   ],
   "source": [
    "if config['final_process']:\n",
    "    post_processing(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_best_acc = str(best_acc).replace('.', '')[:7]\n",
    "# name_best_acc = '_EnsembleLast3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWDf_C-omElb"
   },
   "source": [
    "## Write prediction to a CSV file.\n",
    "\n",
    "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "GuljYSPHcZir"
   },
   "outputs": [],
   "source": [
    "# today = '0326'\n",
    "# CURR_TIME = '1723'\n",
    "def write_csv(predict, today, CURR_TIME, name_best_acc, SHUFFLE):\n",
    "    if SHUFFLE:\n",
    "        file_path = 'prediction_{}___{}_3_shuffle_correction_{}.csv'.format(today, CURR_TIME, name_best_acc)\n",
    "    else:\n",
    "        file_path = 'prediction_{}___{}_3_correction_{}.csv'.format(today, CURR_TIME, name_best_acc)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('Id,Class\\n')\n",
    "        for i, y in enumerate(predict):\n",
    "            f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(result, today, CURR_TIME, name_best_acc, SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# TODO:\n",
    "V _ 3. ensemble when training / ensemble previous model prediction or csv\n",
    " V  4. rewrite model into nn.Sequential\n",
    " V  5. label smoothing\n",
    "    6. CNN (dilated?)\n",
    " V  7. activation function? -> Relu\n",
    " V  8. optimizer (SGD v.s. Adam) -> Adam fast\n",
    "V_V 9. LR scheduler / warm up / look-ahead\n",
    "    10. layer order?\n",
    "        Rule: \n",
    "        * dropout-BN -> Affect\n",
    "        * Act-BN -> Affect \n",
    "        \n",
    "      V (1) FC-BN-dropout-Act (My best)\n",
    "        (2) FC-BN-Act-dropout (Same as (1) ?)\n",
    "        ---\n",
    "        (3) FC-Act-dropout-BN (stack overflow)\n",
    "        (4) FC-dropout-Act-BN (Same as (3) ? but computation cost less than (3)? )\n",
    "        ---\n",
    "      V (5) FC-Act-BN-dropout (Paka)  -> Act before BN would affect\n",
    "     ?? (6) FC-dropout-BN-Act         -> dropout would affect BN\n",
    "        ---\n",
    "      V (7) BN-dropout-FC-Act (paper) -> similar to Paka\n",
    "    11. tsne \n",
    "    12. Plot learning curve\n",
    "    13. Confusion Matrix\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. Weights in Cross Entropy Loss\n",
    "    - https://discuss.pytorch.org/t/weights-in-weighted-loss-nn-crossentropyloss/69514\n",
    "2. Pytorch-toolbox\n",
    "    - https://github.com/PistonY/torch-toolbox#1-labelsmoothingloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(lr = 1e-3, num_epoch = 128, valid_ratio = 0):\n",
    "    train = np.load(data_root + 'train_11.npy')\n",
    "    train_first = train[:, :-39*SKIP_FRAME]\n",
    "    # train_mid = train[:, 39*4:-39*4]\n",
    "    train_last = train[:, 39*SKIP_FRAME:]\n",
    "    train = np.c_[train_first, train_last]\n",
    "    del train_first, train_last\n",
    "\n",
    "    print('Size of training data: {}'.format(train.shape))\n",
    "\n",
    "    train_label = np.load(data_root + 'train_label_11.npy')\n",
    "\n",
    "    VAL_RATIO = valid_ratio\n",
    "\n",
    "    percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "    # train_x, val_x, train_y, val_y = train_test_split(train, train_label, test_size=VAL_RATIO, random_state=139)\n",
    "    train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "    print('Size of training set: {}'.format(train_x.shape))\n",
    "    print('Size of validation set: {}'.format(val_x.shape))\n",
    "\n",
    "    BATCH_SIZE = 8192\n",
    "\n",
    "    train_set = TIMITDataset(train_x, train_y)\n",
    "    val_set = TIMITDataset(val_x, val_y)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    del train, train_label, train_x, train_y, val_x, val_y\n",
    "    gc.collect()\n",
    "\n",
    "    config['optim_hparas']['lr'] = lr\n",
    "    config['num_epoch'] = num_epoch\n",
    "\n",
    "    today, CURR_TIME, best_acc = Train(config, train_loader, val_loader, val_set, model)\n",
    "    return today, CURR_TIME, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# today, CURR_TIME, best_acc = finetune(lr = 1e-2, num_epoch = 1024, valid_ratio = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SHARE MLSpring2021 - HW2-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
